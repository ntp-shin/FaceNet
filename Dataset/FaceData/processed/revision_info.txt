arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25
--------------------
tensorflow version: 2.11.1
--------------------
git hash: b'e4459143bc28cb4022fee9706e7f24f8fb214eda'
--------------------
b'diff --git a/Dataset/FaceData/processed/bounding_boxes_63447.txt b/Dataset/FaceData/processed/bounding_boxes_63447.txt\ndeleted file mode 100644\nindex e313819..0000000\n--- a/Dataset/FaceData/processed/bounding_boxes_63447.txt\n+++ /dev/null\n@@ -1,3 +0,0 @@\n-Dataset/FaceData/processed/sontung/image4.png\n-Dataset/FaceData/processed/ntp/274478890_3152497485072882_1850984654348765700_n.png\n-Dataset/FaceData/processed/ntp/13096354_1610950379227608_9042923751381316751_n.png 22 161 552 762\ndiff --git a/Dataset/FaceData/processed/bounding_boxes_94046.txt b/Dataset/FaceData/processed/bounding_boxes_94046.txt\ndeleted file mode 100644\nindex 2220184..0000000\n--- a/Dataset/FaceData/processed/bounding_boxes_94046.txt\n+++ /dev/null\n@@ -1,68 +0,0 @@\n-Dataset/FaceData/processed/dam/1673336312485.png 444 342 666 614\n-Dataset/FaceData/processed/dam/dvh-ava-1679912707270.png 288 52 516 350\n-Dataset/FaceData/processed/dam/dam-vinh-hung-2.png 404 83 560 280\n-Dataset/FaceData/processed/dam/clip-doc-quyen-dam-vinh-hung-noi-ho-lo-liveshow-qua-suc-kinh-khung-o-san-van-dong-0.png 572 165 735 375\n-Dataset/FaceData/processed/dam/-gay-soc-mang-xa-hoi-damvinhhung1_ozyg_dhdb-1629851911-73-width1000height668.png 419 118 707 492\n-Dataset/FaceData/processed/dam/1625646403880.png 311 42 403 158\n-Dataset/FaceData/processed/dam/6f95a5545394cf4b65830cab489788e8.png 89 36 172 134\n-Dataset/FaceData/processed/dam/82bc8062-5cac-47dc-998d-f11503fa7ca4.png 263 42 392 193\n-Dataset/FaceData/processed/dam/dam-vinh-hung-tim-kiem-nguoi-dong-vai-minh-luc-tre.png 457 79 578 221\n-Dataset/FaceData/processed/dam/damvinhhung-5-1594661074589.png 309 103 457 294\n-Dataset/FaceData/processed/dam/a1480499782-34d4.png 152 67 321 275\n-Dataset/FaceData/processed/dam/21015994640445197356773011189579637090503342n-1629966088318133164726.png 725 94 887 303\n-Dataset/FaceData/processed/dam/1-1676097198871.png 1134 144 1392 499\n-Dataset/FaceData/processed/dam/dam-vinh-hung-la-ai-35express-1200x900.png 328 180 689 621\n-Dataset/FaceData/processed/dam/dam-vinh-hung-3-167991316188358453428-420-387-927-1355-crop-16799132001361927022519.png 231 43 333 176\n-Dataset/FaceData/processed/dam/dam-vinh-hung-tu-nhan-minh-la-thanh-khoc-32-145852.png 321 162 487 363\n-Dataset/FaceData/processed/dam/dam-vinh-hung-01.png 299 204 392 323\n-Dataset/FaceData/processed/dam/hong-muon-nhac-ten-1-nguoi-anh-trong-nghe-hung2-1531534889-width958height959.png 365 100 604 417\n-Dataset/FaceData/processed/dam/1650363937009_600.png 153 87 369 337\n-Dataset/FaceData/processed/ntp/13096354_1610950379227608_9042923751381316751_n.png 22 161 552 762\n-Dataset/FaceData/processed/ntp/217742301_2975344219454877_7910718348071192063_n.png 398 130 693 513\n-Dataset/FaceData/processed/ntp/115974616_2695388184117150_8691131562004983680_n.png 361 112 602 394\n-Dataset/FaceData/processed/ntp/267023530_3096818043974160_8697095605003432314_n.png 162 266 404 524\n-Dataset/FaceData/processed/ntp/149694479_2865665887089378_3567655956748179832_n.png 646 346 765 479\n-Dataset/FaceData/processed/ntp/298004614_3285603365095626_3758753396240895874_n.png 600 697 877 1010\n-Dataset/FaceData/processed/ntp/328153928_855229178906273_2325476014455150249_n.png 489 647 621 808\n-Dataset/FaceData/processed/ntp/274478890_3152497485072882_1850984654348765700_n.png\n-Dataset/FaceData/processed/tran_thanh/272541210717106-1682042259375353277115-1682063148248-16820631483571255261879.png 853 153 1282 723\n-Dataset/FaceData/processed/tran_thanh/anh-1.png 284 78 456 321\n-Dataset/FaceData/processed/tran_thanh/3340929295871259066232977045468368782332470n-1677819651595386286205.png 586 823 795 1077\n-Dataset/FaceData/processed/tran_thanh/150bf009-d342-43a4-96b1-6262a254c2a1-16817895764951720533742.png 845 95 1179 549\n-Dataset/FaceData/processed/tran_thanh/83344795_3250077958354836_1780571850397450240_o.png 151 38 390 363\n-Dataset/FaceData/processed/tran_thanh/322362007-860306168381897-164765490283095007-n-9996.jpeg.png 510 152 820 566\n-Dataset/FaceData/processed/tran_thanh/1561948333051.png 652 25 776 181\n-Dataset/FaceData/processed/tran_thanh/120230303130031.png 258 68 380 220\n-Dataset/FaceData/processed/tran_thanh/anh-t-16794648969421760820329-16795219529011273898351-1679972260463644366129.png 186 0 378 255\n-Dataset/FaceData/processed/tran_thanh/fab2dcef-0a39-4aa7-8bb9-19d7730864d0-16812661451982098911444.png 448 117 735 528\n-Dataset/FaceData/processed/tran_thanh/1-16814680015721478609392.png 252 55 442 311\n-Dataset/FaceData/processed/tran_thanh/3212433598202264457372997396979274374649541n-16778484382331200939974.png 295 233 696 719\n-Dataset/FaceData/processed/sontung/ca-si-son-tung-mtp-4.png 413 157 705 551\n-Dataset/FaceData/processed/sontung/957untitled18-1517211434.png 264 55 418 255\n-Dataset/FaceData/processed/sontung/Chu-ky-Son-Tung.png 315 52 724 578\n-Dataset/FaceData/processed/sontung/image3.png 23 0 70 45\n-Dataset/FaceData/processed/sontung/congly-vn_son-tung-m-tp-gay-bat-ngo-voi-tuyen-bo-nghi-hat-sau-30-tuoi-hinh-anh0820614933.png 322 139 415 254\n-Dataset/FaceData/processed/sontung/5d1eef06a58fa-1200x795.png 520 287 709 526\n-Dataset/FaceData/processed/sontung/image5.png 26 0 77 48\n-Dataset/FaceData/processed/sontung/image7.png 24 0 76 61\n-Dataset/FaceData/processed/sontung/-553e-4dc3-be90-205779b33f5b-1629856734-80-1651241675-714-width650height975.png 203 106 441 400\n-Dataset/FaceData/processed/sontung/220px-Son_Tung_M-TP_2_28201729.png 83 8 157 98\n-Dataset/FaceData/processed/sontung/20679343.png 246 17 396 210\n-Dataset/FaceData/processed/sontung/ca-si-son-tung-m-tp-4aee32b7df234d3c86807cad722a7c4b.png 261 113 698 599\n-Dataset/FaceData/processed/sontung/230px-Sontungmtp.png 81 23 162 122\n-Dataset/FaceData/processed/sontung/image4.png\n-Dataset/FaceData/processed/sontung/image2.png 15 0 89 84\n-Dataset/FaceData/processed/sontung/1620802736418_600.png 228 49 479 388\n-Dataset/FaceData/processed/sontung/anh1_1.png 263 78 417 265\n-Dataset/FaceData/processed/sontung/8e34e3e4a570228fecaf7ce3c81328f0.png 282 73 731 669\n-Dataset/FaceData/processed/sontung/3-6857.png 520 623 612 733\n-Dataset/FaceData/processed/sontung/11707537937887109944759096401674123002289793o-1596615779281752133639-crop-1596615789986723827443.png 184 39 315 215\n-Dataset/FaceData/processed/sontung/9251a048-3167-4b12-99ee-1acc3fe3862a-1633077455-134-width660height660.png 219 42 456 358\n-Dataset/FaceData/processed/sontung/image6.png 18 0 88 68\n-Dataset/FaceData/processed/sontung/79327344_31700559530.png 428 293 526 427\n-Dataset/FaceData/processed/sontung/5277428619431n-16750540557171265505722-1675060346315-16750603465071685418021.png 456 121 602 280\n-Dataset/FaceData/processed/sontung/image.png 73 22 143 104\n-Dataset/FaceData/processed/sontung/2503-mbi-7016-637224755685018659.png 483 105 767 486\n-Dataset/FaceData/processed/sontung/341487093041598.png 323 99 440 247\n-Dataset/FaceData/processed/sontung/174602sontungmtp.png 204 41 403 300\n-Dataset/FaceData/processed/sontung/a-1438493457.png 373 14 851 684\ndiff --git a/Dataset/FaceData/processed/dam/-gay-soc-mang-xa-hoi-damvinhhung1_ozyg_dhdb-1629851911-73-width1000height668.png b/Dataset/FaceData/processed/dam/-gay-soc-mang-xa-hoi-damvinhhung1_ozyg_dhdb-1629851911-73-width1000height668.png\ndeleted file mode 100644\nindex a62dedd..0000000\nBinary files a/Dataset/FaceData/processed/dam/-gay-soc-mang-xa-hoi-damvinhhung1_ozyg_dhdb-1629851911-73-width1000height668.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/1-1676097198871.png b/Dataset/FaceData/processed/dam/1-1676097198871.png\ndeleted file mode 100644\nindex 880fb2b..0000000\nBinary files a/Dataset/FaceData/processed/dam/1-1676097198871.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/1625646403880.png b/Dataset/FaceData/processed/dam/1625646403880.png\ndeleted file mode 100644\nindex 25a70fa..0000000\nBinary files a/Dataset/FaceData/processed/dam/1625646403880.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/1650363937009_600.png b/Dataset/FaceData/processed/dam/1650363937009_600.png\ndeleted file mode 100644\nindex e248009..0000000\nBinary files a/Dataset/FaceData/processed/dam/1650363937009_600.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/1673336312485.png b/Dataset/FaceData/processed/dam/1673336312485.png\ndeleted file mode 100644\nindex 430368c..0000000\nBinary files a/Dataset/FaceData/processed/dam/1673336312485.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/21015994640445197356773011189579637090503342n-1629966088318133164726.png b/Dataset/FaceData/processed/dam/21015994640445197356773011189579637090503342n-1629966088318133164726.png\ndeleted file mode 100644\nindex 565cf8b..0000000\nBinary files a/Dataset/FaceData/processed/dam/21015994640445197356773011189579637090503342n-1629966088318133164726.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/6f95a5545394cf4b65830cab489788e8.png b/Dataset/FaceData/processed/dam/6f95a5545394cf4b65830cab489788e8.png\ndeleted file mode 100644\nindex e597aee..0000000\nBinary files a/Dataset/FaceData/processed/dam/6f95a5545394cf4b65830cab489788e8.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/82bc8062-5cac-47dc-998d-f11503fa7ca4.png b/Dataset/FaceData/processed/dam/82bc8062-5cac-47dc-998d-f11503fa7ca4.png\ndeleted file mode 100644\nindex 9d67b72..0000000\nBinary files a/Dataset/FaceData/processed/dam/82bc8062-5cac-47dc-998d-f11503fa7ca4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/a1480499782-34d4.png b/Dataset/FaceData/processed/dam/a1480499782-34d4.png\ndeleted file mode 100644\nindex adc40b0..0000000\nBinary files a/Dataset/FaceData/processed/dam/a1480499782-34d4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/clip-doc-quyen-dam-vinh-hung-noi-ho-lo-liveshow-qua-suc-kinh-khung-o-san-van-dong-0.png b/Dataset/FaceData/processed/dam/clip-doc-quyen-dam-vinh-hung-noi-ho-lo-liveshow-qua-suc-kinh-khung-o-san-van-dong-0.png\ndeleted file mode 100644\nindex 7040bd4..0000000\nBinary files a/Dataset/FaceData/processed/dam/clip-doc-quyen-dam-vinh-hung-noi-ho-lo-liveshow-qua-suc-kinh-khung-o-san-van-dong-0.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-01.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-01.png\ndeleted file mode 100644\nindex 2772ec2..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-01.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-2.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-2.png\ndeleted file mode 100644\nindex 6fdbc2b..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-2.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-3-167991316188358453428-420-387-927-1355-crop-16799132001361927022519.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-3-167991316188358453428-420-387-927-1355-crop-16799132001361927022519.png\ndeleted file mode 100644\nindex 5931557..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-3-167991316188358453428-420-387-927-1355-crop-16799132001361927022519.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-la-ai-35express-1200x900.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-la-ai-35express-1200x900.png\ndeleted file mode 100644\nindex ac7cb6b..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-la-ai-35express-1200x900.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-tim-kiem-nguoi-dong-vai-minh-luc-tre.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-tim-kiem-nguoi-dong-vai-minh-luc-tre.png\ndeleted file mode 100644\nindex 0bc4ace..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-tim-kiem-nguoi-dong-vai-minh-luc-tre.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dam-vinh-hung-tu-nhan-minh-la-thanh-khoc-32-145852.png b/Dataset/FaceData/processed/dam/dam-vinh-hung-tu-nhan-minh-la-thanh-khoc-32-145852.png\ndeleted file mode 100644\nindex f9119c1..0000000\nBinary files a/Dataset/FaceData/processed/dam/dam-vinh-hung-tu-nhan-minh-la-thanh-khoc-32-145852.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/damvinhhung-5-1594661074589.png b/Dataset/FaceData/processed/dam/damvinhhung-5-1594661074589.png\ndeleted file mode 100644\nindex 016ccb3..0000000\nBinary files a/Dataset/FaceData/processed/dam/damvinhhung-5-1594661074589.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/dvh-ava-1679912707270.png b/Dataset/FaceData/processed/dam/dvh-ava-1679912707270.png\ndeleted file mode 100644\nindex 6f9edad..0000000\nBinary files a/Dataset/FaceData/processed/dam/dvh-ava-1679912707270.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/dam/hong-muon-nhac-ten-1-nguoi-anh-trong-nghe-hung2-1531534889-width958height959.png b/Dataset/FaceData/processed/dam/hong-muon-nhac-ten-1-nguoi-anh-trong-nghe-hung2-1531534889-width958height959.png\ndeleted file mode 100644\nindex f7d023c..0000000\nBinary files a/Dataset/FaceData/processed/dam/hong-muon-nhac-ten-1-nguoi-anh-trong-nghe-hung2-1531534889-width958height959.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/115974616_2695388184117150_8691131562004983680_n.png b/Dataset/FaceData/processed/ntp/115974616_2695388184117150_8691131562004983680_n.png\ndeleted file mode 100644\nindex 586ecdd..0000000\nBinary files a/Dataset/FaceData/processed/ntp/115974616_2695388184117150_8691131562004983680_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/13096354_1610950379227608_9042923751381316751_n.png b/Dataset/FaceData/processed/ntp/13096354_1610950379227608_9042923751381316751_n.png\ndeleted file mode 100644\nindex eae1101..0000000\nBinary files a/Dataset/FaceData/processed/ntp/13096354_1610950379227608_9042923751381316751_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/149694479_2865665887089378_3567655956748179832_n.png b/Dataset/FaceData/processed/ntp/149694479_2865665887089378_3567655956748179832_n.png\ndeleted file mode 100644\nindex 2c887b4..0000000\nBinary files a/Dataset/FaceData/processed/ntp/149694479_2865665887089378_3567655956748179832_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/217742301_2975344219454877_7910718348071192063_n.png b/Dataset/FaceData/processed/ntp/217742301_2975344219454877_7910718348071192063_n.png\ndeleted file mode 100644\nindex 06acc5e..0000000\nBinary files a/Dataset/FaceData/processed/ntp/217742301_2975344219454877_7910718348071192063_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/267023530_3096818043974160_8697095605003432314_n.png b/Dataset/FaceData/processed/ntp/267023530_3096818043974160_8697095605003432314_n.png\ndeleted file mode 100644\nindex f6b5093..0000000\nBinary files a/Dataset/FaceData/processed/ntp/267023530_3096818043974160_8697095605003432314_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/298004614_3285603365095626_3758753396240895874_n.png b/Dataset/FaceData/processed/ntp/298004614_3285603365095626_3758753396240895874_n.png\ndeleted file mode 100644\nindex d27b44a..0000000\nBinary files a/Dataset/FaceData/processed/ntp/298004614_3285603365095626_3758753396240895874_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/ntp/328153928_855229178906273_2325476014455150249_n.png b/Dataset/FaceData/processed/ntp/328153928_855229178906273_2325476014455150249_n.png\ndeleted file mode 100644\nindex bad26f9..0000000\nBinary files a/Dataset/FaceData/processed/ntp/328153928_855229178906273_2325476014455150249_n.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/revision_info.txt b/Dataset/FaceData/processed/revision_info.txt\ndeleted file mode 100644\nindex 3e17574..0000000\n--- a/Dataset/FaceData/processed/revision_info.txt\n+++ /dev/null\n@@ -1,7 +0,0 @@\n-arguments: src/align_dataset_mtcnn.py Dataset/FaceData/raw Dataset/FaceData/processed --image_size 160 --margin 32 --random_order --gpu_memory_fraction 0.25\n---------------------\n-tensorflow version: 2.11.1\n---------------------\n-git hash: b\'69ff1e149c0d84a123d6516ddd82970e65392608\'\n---------------------\n-b\'diff --git a/requirements.txt b/requirements.txt\\nindex aa078a0..377b333 100644\\n--- a/requirements.txt\\n+++ b/requirements.txt\\n@@ -1,9 +1,4 @@\\n-tensorflow\\n-keras\\n-scikit-learn\\n-opencv-python\\n h5py\\n-matplotlib\\n Pillow\\n requests\\n psutil\\ndiff --git a/src/face_rec.py b/src/face_rec.py\\nindex f92cccf..18dd68b 100644\\n--- a/src/face_rec.py\\n+++ b/src/face_rec.py\\n@@ -49,9 +49,10 @@ def main():\\n             facenet.load_model(FACENET_MODEL_PATH)\\n \\n             # Lay tensor input va output\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\n+\\n             embedding_size = embeddings.get_shape()[1]\\n \\n             # Cai dat cac mang con\\ndiff --git a/src/face_rec_cam.py b/src/face_rec_cam.py\\nindex 1a425a5..907a382 100644\\n--- a/src/face_rec_cam.py\\n+++ b/src/face_rec_cam.py\\n@@ -52,9 +52,10 @@ def main():\\n             facenet.load_model(FACENET_MODEL_PATH)\\n \\n             # Get input and output tensors\\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\\n+            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\\n+            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\\n+            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\\n+\\n             embedding_size = embeddings.get_shape()[1]\\n \\n             pnet, rnet, onet = align.detect_face.create_mtcnn(sess, "src/align")\'\n\\ No newline at end of file\ndiff --git a/Dataset/FaceData/processed/sontung/-553e-4dc3-be90-205779b33f5b-1629856734-80-1651241675-714-width650height975.png b/Dataset/FaceData/processed/sontung/-553e-4dc3-be90-205779b33f5b-1629856734-80-1651241675-714-width650height975.png\ndeleted file mode 100644\nindex 10bdc04..0000000\nBinary files a/Dataset/FaceData/processed/sontung/-553e-4dc3-be90-205779b33f5b-1629856734-80-1651241675-714-width650height975.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/11707537937887109944759096401674123002289793o-1596615779281752133639-crop-1596615789986723827443.png b/Dataset/FaceData/processed/sontung/11707537937887109944759096401674123002289793o-1596615779281752133639-crop-1596615789986723827443.png\ndeleted file mode 100644\nindex f814b86..0000000\nBinary files a/Dataset/FaceData/processed/sontung/11707537937887109944759096401674123002289793o-1596615779281752133639-crop-1596615789986723827443.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/1620802736418_600.png b/Dataset/FaceData/processed/sontung/1620802736418_600.png\ndeleted file mode 100644\nindex 68aefc6..0000000\nBinary files a/Dataset/FaceData/processed/sontung/1620802736418_600.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/174602sontungmtp.png b/Dataset/FaceData/processed/sontung/174602sontungmtp.png\ndeleted file mode 100644\nindex 65b2300..0000000\nBinary files a/Dataset/FaceData/processed/sontung/174602sontungmtp.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/20679343.png b/Dataset/FaceData/processed/sontung/20679343.png\ndeleted file mode 100644\nindex f8d4a59..0000000\nBinary files a/Dataset/FaceData/processed/sontung/20679343.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/220px-Son_Tung_M-TP_2_28201729.png b/Dataset/FaceData/processed/sontung/220px-Son_Tung_M-TP_2_28201729.png\ndeleted file mode 100644\nindex 263e28d..0000000\nBinary files a/Dataset/FaceData/processed/sontung/220px-Son_Tung_M-TP_2_28201729.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/230px-Sontungmtp.png b/Dataset/FaceData/processed/sontung/230px-Sontungmtp.png\ndeleted file mode 100644\nindex eacf403..0000000\nBinary files a/Dataset/FaceData/processed/sontung/230px-Sontungmtp.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/2503-mbi-7016-637224755685018659.png b/Dataset/FaceData/processed/sontung/2503-mbi-7016-637224755685018659.png\ndeleted file mode 100644\nindex 9ea8ce3..0000000\nBinary files a/Dataset/FaceData/processed/sontung/2503-mbi-7016-637224755685018659.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/3-6857.png b/Dataset/FaceData/processed/sontung/3-6857.png\ndeleted file mode 100644\nindex fe61827..0000000\nBinary files a/Dataset/FaceData/processed/sontung/3-6857.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/341487093041598.png b/Dataset/FaceData/processed/sontung/341487093041598.png\ndeleted file mode 100644\nindex 2ec2bc4..0000000\nBinary files a/Dataset/FaceData/processed/sontung/341487093041598.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/5277428619431n-16750540557171265505722-1675060346315-16750603465071685418021.png b/Dataset/FaceData/processed/sontung/5277428619431n-16750540557171265505722-1675060346315-16750603465071685418021.png\ndeleted file mode 100644\nindex 7e10501..0000000\nBinary files a/Dataset/FaceData/processed/sontung/5277428619431n-16750540557171265505722-1675060346315-16750603465071685418021.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/5d1eef06a58fa-1200x795.png b/Dataset/FaceData/processed/sontung/5d1eef06a58fa-1200x795.png\ndeleted file mode 100644\nindex e634895..0000000\nBinary files a/Dataset/FaceData/processed/sontung/5d1eef06a58fa-1200x795.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/79327344_31700559530.png b/Dataset/FaceData/processed/sontung/79327344_31700559530.png\ndeleted file mode 100644\nindex aa2bae1..0000000\nBinary files a/Dataset/FaceData/processed/sontung/79327344_31700559530.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/8e34e3e4a570228fecaf7ce3c81328f0.png b/Dataset/FaceData/processed/sontung/8e34e3e4a570228fecaf7ce3c81328f0.png\ndeleted file mode 100644\nindex e7eb2af..0000000\nBinary files a/Dataset/FaceData/processed/sontung/8e34e3e4a570228fecaf7ce3c81328f0.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/9251a048-3167-4b12-99ee-1acc3fe3862a-1633077455-134-width660height660.png b/Dataset/FaceData/processed/sontung/9251a048-3167-4b12-99ee-1acc3fe3862a-1633077455-134-width660height660.png\ndeleted file mode 100644\nindex e6b9748..0000000\nBinary files a/Dataset/FaceData/processed/sontung/9251a048-3167-4b12-99ee-1acc3fe3862a-1633077455-134-width660height660.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/957untitled18-1517211434.png b/Dataset/FaceData/processed/sontung/957untitled18-1517211434.png\ndeleted file mode 100644\nindex a187d16..0000000\nBinary files a/Dataset/FaceData/processed/sontung/957untitled18-1517211434.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/Chu-ky-Son-Tung.png b/Dataset/FaceData/processed/sontung/Chu-ky-Son-Tung.png\ndeleted file mode 100644\nindex 8f85f73..0000000\nBinary files a/Dataset/FaceData/processed/sontung/Chu-ky-Son-Tung.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/a-1438493457.png b/Dataset/FaceData/processed/sontung/a-1438493457.png\ndeleted file mode 100644\nindex ff53582..0000000\nBinary files a/Dataset/FaceData/processed/sontung/a-1438493457.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/anh1_1.png b/Dataset/FaceData/processed/sontung/anh1_1.png\ndeleted file mode 100644\nindex a0a32ff..0000000\nBinary files a/Dataset/FaceData/processed/sontung/anh1_1.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/ca-si-son-tung-m-tp-4aee32b7df234d3c86807cad722a7c4b.png b/Dataset/FaceData/processed/sontung/ca-si-son-tung-m-tp-4aee32b7df234d3c86807cad722a7c4b.png\ndeleted file mode 100644\nindex 0c1237a..0000000\nBinary files a/Dataset/FaceData/processed/sontung/ca-si-son-tung-m-tp-4aee32b7df234d3c86807cad722a7c4b.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/ca-si-son-tung-mtp-4.png b/Dataset/FaceData/processed/sontung/ca-si-son-tung-mtp-4.png\ndeleted file mode 100644\nindex 6fc02dd..0000000\nBinary files a/Dataset/FaceData/processed/sontung/ca-si-son-tung-mtp-4.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/congly-vn_son-tung-m-tp-gay-bat-ngo-voi-tuyen-bo-nghi-hat-sau-30-tuoi-hinh-anh0820614933.png b/Dataset/FaceData/processed/sontung/congly-vn_son-tung-m-tp-gay-bat-ngo-voi-tuyen-bo-nghi-hat-sau-30-tuoi-hinh-anh0820614933.png\ndeleted file mode 100644\nindex dcc1a5d..0000000\nBinary files a/Dataset/FaceData/processed/sontung/congly-vn_son-tung-m-tp-gay-bat-ngo-voi-tuyen-bo-nghi-hat-sau-30-tuoi-hinh-anh0820614933.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image.png b/Dataset/FaceData/processed/sontung/image.png\ndeleted file mode 100644\nindex 2e8492d..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image2.png b/Dataset/FaceData/processed/sontung/image2.png\ndeleted file mode 100644\nindex c672973..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image2.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image3.png b/Dataset/FaceData/processed/sontung/image3.png\ndeleted file mode 100644\nindex 13f9aa9..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image3.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image5.png b/Dataset/FaceData/processed/sontung/image5.png\ndeleted file mode 100644\nindex b619215..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image5.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image6.png b/Dataset/FaceData/processed/sontung/image6.png\ndeleted file mode 100644\nindex 86c68b0..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image6.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/sontung/image7.png b/Dataset/FaceData/processed/sontung/image7.png\ndeleted file mode 100644\nindex 6ebd5cc..0000000\nBinary files a/Dataset/FaceData/processed/sontung/image7.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/1-16814680015721478609392.png b/Dataset/FaceData/processed/tran_thanh/1-16814680015721478609392.png\ndeleted file mode 100644\nindex 967d879..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/1-16814680015721478609392.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/120230303130031.png b/Dataset/FaceData/processed/tran_thanh/120230303130031.png\ndeleted file mode 100644\nindex b79d672..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/120230303130031.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/150bf009-d342-43a4-96b1-6262a254c2a1-16817895764951720533742.png b/Dataset/FaceData/processed/tran_thanh/150bf009-d342-43a4-96b1-6262a254c2a1-16817895764951720533742.png\ndeleted file mode 100644\nindex fa639da..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/150bf009-d342-43a4-96b1-6262a254c2a1-16817895764951720533742.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/1561948333051.png b/Dataset/FaceData/processed/tran_thanh/1561948333051.png\ndeleted file mode 100644\nindex e7cc08f..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/1561948333051.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/272541210717106-1682042259375353277115-1682063148248-16820631483571255261879.png b/Dataset/FaceData/processed/tran_thanh/272541210717106-1682042259375353277115-1682063148248-16820631483571255261879.png\ndeleted file mode 100644\nindex 15e678f..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/272541210717106-1682042259375353277115-1682063148248-16820631483571255261879.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/3212433598202264457372997396979274374649541n-16778484382331200939974.png b/Dataset/FaceData/processed/tran_thanh/3212433598202264457372997396979274374649541n-16778484382331200939974.png\ndeleted file mode 100644\nindex e7ea419..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/3212433598202264457372997396979274374649541n-16778484382331200939974.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/322362007-860306168381897-164765490283095007-n-9996.jpeg.png b/Dataset/FaceData/processed/tran_thanh/322362007-860306168381897-164765490283095007-n-9996.jpeg.png\ndeleted file mode 100644\nindex 850c2f3..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/322362007-860306168381897-164765490283095007-n-9996.jpeg.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/3340929295871259066232977045468368782332470n-1677819651595386286205.png b/Dataset/FaceData/processed/tran_thanh/3340929295871259066232977045468368782332470n-1677819651595386286205.png\ndeleted file mode 100644\nindex 7b6ae77..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/3340929295871259066232977045468368782332470n-1677819651595386286205.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/83344795_3250077958354836_1780571850397450240_o.png b/Dataset/FaceData/processed/tran_thanh/83344795_3250077958354836_1780571850397450240_o.png\ndeleted file mode 100644\nindex 0a097ad..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/83344795_3250077958354836_1780571850397450240_o.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/anh-1.png b/Dataset/FaceData/processed/tran_thanh/anh-1.png\ndeleted file mode 100644\nindex 2b4b275..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/anh-1.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/anh-t-16794648969421760820329-16795219529011273898351-1679972260463644366129.png b/Dataset/FaceData/processed/tran_thanh/anh-t-16794648969421760820329-16795219529011273898351-1679972260463644366129.png\ndeleted file mode 100644\nindex e242f1c..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/anh-t-16794648969421760820329-16795219529011273898351-1679972260463644366129.png and /dev/null differ\ndiff --git a/Dataset/FaceData/processed/tran_thanh/fab2dcef-0a39-4aa7-8bb9-19d7730864d0-16812661451982098911444.png b/Dataset/FaceData/processed/tran_thanh/fab2dcef-0a39-4aa7-8bb9-19d7730864d0-16812661451982098911444.png\ndeleted file mode 100644\nindex f80766a..0000000\nBinary files a/Dataset/FaceData/processed/tran_thanh/fab2dcef-0a39-4aa7-8bb9-19d7730864d0-16812661451982098911444.png and /dev/null differ\ndiff --git a/src/__pycache__/facenet.cpython-39.pyc b/src/__pycache__/facenet.cpython-39.pyc\nindex 0f14f61..13495bd 100644\nBinary files a/src/__pycache__/facenet.cpython-39.pyc and b/src/__pycache__/facenet.cpython-39.pyc differ\ndiff --git a/src/align/__pycache__/detect_face.cpython-39.pyc b/src/align/__pycache__/detect_face.cpython-39.pyc\nindex 6747e75..1437fd8 100644\nBinary files a/src/align/__pycache__/detect_face.cpython-39.pyc and b/src/align/__pycache__/detect_face.cpython-39.pyc differ\ndiff --git a/src/calculate_filtering_metrics.py b/src/calculate_filtering_metrics.py\ndeleted file mode 100644\nindex f60b9ae..0000000\n--- a/src/calculate_filtering_metrics.py\n+++ /dev/null\n@@ -1,128 +0,0 @@\n-"""Calculate filtering metrics for a dataset and store in a .hdf file.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import os\n-import sys\n-import time\n-import h5py\n-import math\n-from tensorflow.python.platform import gfile\n-from six import iteritems\n-\n-def main(args):\n-    dataset = facenet.get_dataset(args.dataset_dir)\n-  \n-    with tf.Graph().as_default():\n-      \n-        # Get a list of image paths and their labels\n-        image_list, label_list = facenet.get_image_paths_and_labels(dataset)\n-        nrof_images = len(image_list)\n-        image_indices = range(nrof_images)\n-\n-        image_batch, label_batch = facenet.read_and_augment_data(image_list,\n-            image_indices, args.image_size, args.batch_size, None, \n-            False, False, False, nrof_preprocess_threads=4, shuffle=False)\n-        \n-        model_exp = os.path.expanduser(args.model_file)\n-        with gfile.FastGFile(model_exp,\'rb\') as f:\n-            graph_def = tf.GraphDef()\n-            graph_def.ParseFromString(f.read())\n-            input_map={\'input\':image_batch, \'phase_train\':False}\n-            tf.import_graph_def(graph_def, input_map=input_map, name=\'net\')\n-        \n-        embeddings = tf.get_default_graph().get_tensor_by_name("net/embeddings:0")\n-\n-        with tf.Session() as sess:\n-            tf.train.start_queue_runners(sess=sess)\n-                \n-            embedding_size = int(embeddings.get_shape()[1])\n-            nrof_batches = int(math.ceil(nrof_images / args.batch_size))\n-            nrof_classes = len(dataset)\n-            label_array = np.array(label_list)\n-            class_names = [cls.name for cls in dataset]\n-            nrof_examples_per_class = [ len(cls.image_paths) for cls in dataset ]\n-            class_variance = np.zeros((nrof_classes,))\n-            class_center = np.zeros((nrof_classes,embedding_size))\n-            distance_to_center = np.ones((len(label_list),))*np.NaN\n-            emb_array = np.zeros((0,embedding_size))\n-            idx_array = np.zeros((0,), dtype=np.int32)\n-            lab_array = np.zeros((0,), dtype=np.int32)\n-            index_arr = np.append(0, np.cumsum(nrof_examples_per_class))\n-            for i in range(nrof_batches):\n-                t = time.time()\n-                emb, idx = sess.run([embeddings, label_batch])\n-                emb_array = np.append(emb_array, emb, axis=0)\n-                idx_array = np.append(idx_array, idx, axis=0)\n-                lab_array = np.append(lab_array, label_array[idx], axis=0)\n-                for cls in set(lab_array):\n-                    cls_idx = np.where(lab_array==cls)[0]\n-                    if cls_idx.shape[0]==nrof_examples_per_class[cls]:\n-                        # We have calculated all the embeddings for this class\n-                        i2 = np.argsort(idx_array[cls_idx])\n-                        emb_class = emb_array[cls_idx,:]\n-                        emb_sort = emb_class[i2,:]\n-                        center = np.mean(emb_sort, axis=0)\n-                        diffs = emb_sort - center\n-                        dists_sqr = np.sum(np.square(diffs), axis=1)\n-                        class_variance[cls] = np.mean(dists_sqr)\n-                        class_center[cls,:] = center\n-                        distance_to_center[index_arr[cls]:index_arr[cls+1]] = np.sqrt(dists_sqr)\n-                        emb_array = np.delete(emb_array, cls_idx, axis=0)\n-                        idx_array = np.delete(idx_array, cls_idx, axis=0)\n-                        lab_array = np.delete(lab_array, cls_idx, axis=0)\n-\n-                        \n-                print(\'Batch %d in %.3f seconds\' % (i, time.time()-t))\n-                \n-            print(\'Writing filtering data to %s\' % args.data_file_name)\n-            mdict = {\'class_names\':class_names, \'image_list\':image_list, \'label_list\':label_list, \'distance_to_center\':distance_to_center }\n-            with h5py.File(args.data_file_name, \'w\') as f:\n-                for key, value in iteritems(mdict):\n-                    f.create_dataset(key, data=value)\n-                        \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'dataset_dir\', type=str,\n-        help=\'Path to the directory containing aligned dataset.\')\n-    parser.add_argument(\'model_file\', type=str,\n-        help=\'File containing the frozen model in protobuf (.pb) format to use for feature extraction.\')\n-    parser.add_argument(\'data_file_name\', type=str,\n-        help=\'The name of the file to store filtering data in.\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size.\', default=160)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/classifier.py b/src/classifier.py\ndeleted file mode 100644\nindex 6a176a4..0000000\n--- a/src/classifier.py\n+++ /dev/null\n@@ -1,170 +0,0 @@\n-"""An example of how to use your own dataset to train a classifier that recognizes people.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import os\n-import sys\n-import math\n-import pickle\n-from sklearn.svm import SVC\n-\n-def main(args):\n-  \n-    with tf.Graph().as_default():\n-      \n-        with tf.compat.v1.Session() as sess:\n-            \n-            np.random.seed(seed=args.seed)\n-            \n-            if args.use_split_dataset:\n-                dataset_tmp = facenet.get_dataset(args.data_dir)\n-                train_set, test_set = split_dataset(dataset_tmp, args.min_nrof_images_per_class, args.nrof_train_images_per_class)\n-                if (args.mode==\'TRAIN\'):\n-                    dataset = train_set\n-                elif (args.mode==\'CLASSIFY\'):\n-                    dataset = test_set\n-            else:\n-                dataset = facenet.get_dataset(args.data_dir)\n-\n-            # Check that there are at least one training image per class\n-            for cls in dataset:\n-                assert(len(cls.image_paths)>0, \'There must be at least one image for each class in the dataset\')\n-\n-                 \n-            paths, labels = facenet.get_image_paths_and_labels(dataset)\n-            \n-            print(\'Number of classes: %d\' % len(dataset))\n-            print(\'Number of images: %d\' % len(paths))\n-            \n-            # Load the model\n-            print(\'Loading feature extraction model\')\n-            facenet.load_model(args.model)\n-            \n-            # Get input and output tensors\n-            images_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.compat.v1.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.compat.v1.get_default_graph().get_tensor_by_name("phase_train:0")\n-            embedding_size = embeddings.get_shape()[1]\n-            \n-            # Run forward pass to calculate embeddings\n-            print(\'Calculating features for images\')\n-            nrof_images = len(paths)\n-            nrof_batches_per_epoch = int(math.ceil(1.0*nrof_images / args.batch_size))\n-            emb_array = np.zeros((nrof_images, embedding_size))\n-            for i in range(nrof_batches_per_epoch):\n-                start_index = i*args.batch_size\n-                end_index = min((i+1)*args.batch_size, nrof_images)\n-                paths_batch = paths[start_index:end_index]\n-                images = facenet.load_data(paths_batch, False, False, args.image_size)\n-                feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n-                emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            classifier_filename_exp = os.path.expanduser(args.classifier_filename)\n-\n-            if (args.mode==\'TRAIN\'):\n-                # Train classifier\n-                print(\'Training classifier\')\n-                model = SVC(kernel=\'linear\', probability=True)\n-                model.fit(emb_array, labels)\n-            \n-                # Create a list of class names\n-                class_names = [ cls.name.replace(\'_\', \' \') for cls in dataset]\n-\n-                # Saving classifier model\n-                with open(classifier_filename_exp, \'wb\') as outfile:\n-                    pickle.dump((model, class_names), outfile)\n-                print(\'Saved classifier model to file "%s"\' % classifier_filename_exp)\n-                \n-            elif (args.mode==\'CLASSIFY\'):\n-                # Classify images\n-                print(\'Testing classifier\')\n-                with open(classifier_filename_exp, \'rb\') as infile:\n-                    (model, class_names) = pickle.load(infile)\n-\n-                print(\'Loaded classifier model from file "%s"\' % classifier_filename_exp)\n-\n-                predictions = model.predict_proba(emb_array)\n-                best_class_indices = np.argmax(predictions, axis=1)\n-                best_class_probabilities = predictions[np.arange(len(best_class_indices)), best_class_indices]\n-                \n-                for i in range(len(best_class_indices)):\n-                    print(\'%4d  %s: %.3f\' % (i, class_names[best_class_indices[i]], best_class_probabilities[i]))\n-                    \n-                accuracy = np.mean(np.equal(best_class_indices, labels))\n-                print(\'Accuracy: %.3f\' % accuracy)\n-                \n-            \n-def split_dataset(dataset, min_nrof_images_per_class, nrof_train_images_per_class):\n-    train_set = []\n-    test_set = []\n-    for cls in dataset:\n-        paths = cls.image_paths\n-        # Remove classes with less than min_nrof_images_per_class\n-        if len(paths)>=min_nrof_images_per_class:\n-            np.random.shuffle(paths)\n-            train_set.append(facenet.ImageClass(cls.name, paths[:nrof_train_images_per_class]))\n-            test_set.append(facenet.ImageClass(cls.name, paths[nrof_train_images_per_class:]))\n-    return train_set, test_set\n-\n-            \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'mode\', type=str, choices=[\'TRAIN\', \'CLASSIFY\'],\n-        help=\'Indicates if a new classifier should be trained or a classification \' + \n-        \'model should be used for classification\', default=\'CLASSIFY\')\n-    parser.add_argument(\'data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'classifier_filename\', \n-        help=\'Classifier model file name as a pickle (.pkl) file. \' + \n-        \'For training this is the output and for classification this is an input.\')\n-    parser.add_argument(\'--use_split_dataset\', \n-        help=\'Indicates that the dataset specified by data_dir should be split into a training and test set. \' +  \n-        \'Otherwise a separate test set can be specified using the test_data_dir option.\', action=\'store_true\')\n-    parser.add_argument(\'--test_data_dir\', type=str,\n-        help=\'Path to the test data directory containing aligned images used for testing.\')\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--min_nrof_images_per_class\', type=int,\n-        help=\'Only include classes with at least this number of images in the dataset\', default=20)\n-    parser.add_argument(\'--nrof_train_images_per_class\', type=int,\n-        help=\'Use this number of images from each class for training and the rest for testing\', default=10)\n-    \n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/compare.py b/src/compare.py\ndeleted file mode 100644\nindex bc53cc4..0000000\n--- a/src/compare.py\n+++ /dev/null\n@@ -1,130 +0,0 @@\n-"""Performs face alignment and calculates L2 distance between the embeddings of images."""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import tensorflow as tf\n-import numpy as np\n-import sys\n-import os\n-import copy\n-import argparse\n-import facenet\n-import align.detect_face\n-\n-def main(args):\n-\n-    images = load_and_align_data(args.image_files, args.image_size, args.margin, args.gpu_memory_fraction)\n-    with tf.Graph().as_default():\n-\n-        with tf.Session() as sess:\n-      \n-            # Load the model\n-            facenet.load_model(args.model)\n-    \n-            # Get input and output tensors\n-            images_placeholder = tf.get_default_graph().get_tensor_by_name("input:0")\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-            phase_train_placeholder = tf.get_default_graph().get_tensor_by_name("phase_train:0")\n-\n-            # Run forward pass to calculate embeddings\n-            feed_dict = { images_placeholder: images, phase_train_placeholder:False }\n-            emb = sess.run(embeddings, feed_dict=feed_dict)\n-            \n-            nrof_images = len(args.image_files)\n-\n-            print(\'Images:\')\n-            for i in range(nrof_images):\n-                print(\'%1d: %s\' % (i, args.image_files[i]))\n-            print(\'\')\n-            \n-            # Print distance matrix\n-            print(\'Distance matrix\')\n-            print(\'    \', end=\'\')\n-            for i in range(nrof_images):\n-                print(\'    %1d     \' % i, end=\'\')\n-            print(\'\')\n-            for i in range(nrof_images):\n-                print(\'%1d  \' % i, end=\'\')\n-                for j in range(nrof_images):\n-                    dist = np.sqrt(np.sum(np.square(np.subtract(emb[i,:], emb[j,:]))))\n-                    print(\'  %1.4f  \' % dist, end=\'\')\n-                print(\'\')\n-            \n-            \n-def load_and_align_data(image_paths, image_size, margin, gpu_memory_fraction):\n-\n-    minsize = 20 # minimum size of face\n-    threshold = [ 0.6, 0.7, 0.7 ]  # three steps\'s threshold\n-    factor = 0.709 # scale factor\n-    \n-    print(\'Creating networks and loading parameters\')\n-    with tf.Graph().as_default():\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        with sess.as_default():\n-            pnet, rnet, onet = align.detect_face.create_mtcnn(sess, None)\n-  \n-    tmp_image_paths=copy.copy(image_paths)\n-    img_list = []\n-    for image in tmp_image_paths:\n-        img = misc.imread(os.path.expanduser(image), mode=\'RGB\')\n-        img_size = np.asarray(img.shape)[0:2]\n-        bounding_boxes, _ = align.detect_face.detect_face(img, minsize, pnet, rnet, onet, threshold, factor)\n-        if len(bounding_boxes) < 1:\n-          image_paths.remove(image)\n-          print("can\'t detect face, remove ", image)\n-          continue\n-        det = np.squeeze(bounding_boxes[0,0:4])\n-        bb = np.zeros(4, dtype=np.int32)\n-        bb[0] = np.maximum(det[0]-margin/2, 0)\n-        bb[1] = np.maximum(det[1]-margin/2, 0)\n-        bb[2] = np.minimum(det[2]+margin/2, img_size[1])\n-        bb[3] = np.minimum(det[3]+margin/2, img_size[0])\n-        cropped = img[bb[1]:bb[3],bb[0]:bb[2],:]\n-        aligned = misc.imresize(cropped, (image_size, image_size), interp=\'bilinear\')\n-        prewhitened = facenet.prewhiten(aligned)\n-        img_list.append(prewhitened)\n-    images = np.stack(img_list)\n-    return images\n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'image_files\', type=str, nargs=\'+\', help=\'Images to compare\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--margin\', type=int,\n-        help=\'Margin for the crop around the bounding box (height, width) in pixels.\', default=44)\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/decode_msceleb_dataset.py b/src/decode_msceleb_dataset.py\ndeleted file mode 100644\nindex 4556bfa..0000000\n--- a/src/decode_msceleb_dataset.py\n+++ /dev/null\n@@ -1,87 +0,0 @@\n-"""Decode the MsCelebV1 dataset in TSV (tab separated values) format downloaded from\n-https://www.microsoft.com/en-us/research/project/ms-celeb-1m-challenge-recognizing-one-million-celebrities-real-world/\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from scipy import misc\n-import numpy as np\n-import base64\n-import sys\n-import os\n-import cv2\n-import argparse\n-import facenet\n-\n-\n-# File format: text files, each line is an image record containing 6 columns, delimited by TAB.\n-# Column1: Freebase MID\n-# Column2: Query/Name\n-# Column3: ImageSearchRank\n-# Column4: ImageURL\n-# Column5: PageURL\n-# Column6: ImageData_Base64Encoded\n-\n-def main(args):\n-    output_dir = os.path.expanduser(args.output_dir)\n-  \n-    if not os.path.exists(output_dir):\n-        os.mkdir(output_dir)\n-  \n-    # Store some git revision info in a text file in the output directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, output_dir, \' \'.join(sys.argv))\n-    \n-    i = 0\n-    for f in args.tsv_files:\n-        for line in f:\n-            fields = line.split(\'\\t\')\n-            class_dir = fields[0]\n-            img_name = fields[1] + \'-\' + fields[4] + \'.\' + args.output_format\n-            img_string = fields[5]\n-            img_dec_string = base64.b64decode(img_string)\n-            img_data = np.fromstring(img_dec_string, dtype=np.uint8)\n-            img = cv2.imdecode(img_data, cv2.IMREAD_COLOR) #pylint: disable=maybe-no-member\n-            if args.size:\n-                img = misc.imresize(img, (args.size, args.size), interp=\'bilinear\')\n-            full_class_dir = os.path.join(output_dir, class_dir)\n-            if not os.path.exists(full_class_dir):\n-                os.mkdir(full_class_dir)\n-            full_path = os.path.join(full_class_dir, img_name.replace(\'/\',\'_\'))\n-            cv2.imwrite(full_path, img) #pylint: disable=maybe-no-member\n-            print(\'%8d: %s\' % (i, full_path))\n-            i += 1\n-  \n-if __name__ == \'__main__\':\n-    parser = argparse.ArgumentParser()\n-\n-    parser.add_argument(\'output_dir\', type=str, help=\'Output base directory for the image dataset\')\n-    parser.add_argument(\'tsv_files\', type=argparse.FileType(\'r\'), nargs=\'+\', help=\'Input TSV file name(s)\')\n-    parser.add_argument(\'--size\', type=int, help=\'Images are resized to the given size\')\n-    parser.add_argument(\'--output_format\', type=str, help=\'Format of the output images\', default=\'png\', choices=[\'png\', \'jpg\'])\n-\n-    main(parser.parse_args())\n-\ndiff --git a/src/download_and_extract.py b/src/download_and_extract.py\ndeleted file mode 100644\nindex a835ac2..0000000\n--- a/src/download_and_extract.py\n+++ /dev/null\n@@ -1,51 +0,0 @@\n-import requests\n-import zipfile\n-import os\n-\n-model_dict = {\n-    \'lfw-subset\':      \'1B5BQUZuJO-paxdN8UclxeHAR1WnR_Tzi\', \n-    \'20170131-234652\': \'0B5MzpY9kBtDVSGM0RmVET2EwVEk\',\n-    \'20170216-091149\': \'0B5MzpY9kBtDVTGZjcWkzT3pldDA\',\n-    \'20170512-110547\': \'0B5MzpY9kBtDVZ2RpVDYwWmxoSUk\',\n-    \'20180402-114759\': \'1EXPBSXwTaqrSC0OhUdXNmKSh9qJUQ55-\'\n-    }\n-\n-def download_and_extract_file(model_name, data_dir):\n-    file_id = model_dict[model_name]\n-    destination = os.path.join(data_dir, model_name + \'.zip\')\n-    if not os.path.exists(destination):\n-        print(\'Downloading file to %s\' % destination)\n-        download_file_from_google_drive(file_id, destination)\n-        with zipfile.ZipFile(destination, \'r\') as zip_ref:\n-            print(\'Extracting file to %s\' % data_dir)\n-            zip_ref.extractall(data_dir)\n-\n-def download_file_from_google_drive(file_id, destination):\n-    \n-        URL = "https://drive.google.com/uc?export=download"\n-    \n-        session = requests.Session()\n-    \n-        response = session.get(URL, params = { \'id\' : file_id }, stream = True)\n-        token = get_confirm_token(response)\n-    \n-        if token:\n-            params = { \'id\' : file_id, \'confirm\' : token }\n-            response = session.get(URL, params = params, stream = True)\n-    \n-        save_response_content(response, destination)    \n-\n-def get_confirm_token(response):\n-    for key, value in response.cookies.items():\n-        if key.startswith(\'download_warning\'):\n-            return value\n-\n-    return None\n-\n-def save_response_content(response, destination):\n-    CHUNK_SIZE = 32768\n-\n-    with open(destination, "wb") as f:\n-        for chunk in response.iter_content(CHUNK_SIZE):\n-            if chunk: # filter out keep-alive new chunks\n-                f.write(chunk)\ndiff --git a/src/freeze_graph.py b/src/freeze_graph.py\ndeleted file mode 100644\nindex 3584c18..0000000\n--- a/src/freeze_graph.py\n+++ /dev/null\n@@ -1,103 +0,0 @@\n-"""Imports a model metagraph and checkpoint file, converts the variables to constants\n-and exports the model as a graphdef protobuf\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from tensorflow.python.framework import graph_util\n-import tensorflow as tf\n-import argparse\n-import os\n-import sys\n-import facenet\n-from six.moves import xrange  # @UnresolvedImport\n-\n-def main(args):\n-    with tf.Graph().as_default():\n-        with tf.Session() as sess:\n-            # Load the model metagraph and checkpoint\n-            print(\'Model directory: %s\' % args.model_dir)\n-            meta_file, ckpt_file = facenet.get_model_filenames(os.path.expanduser(args.model_dir))\n-            \n-            print(\'Metagraph file: %s\' % meta_file)\n-            print(\'Checkpoint file: %s\' % ckpt_file)\n-\n-            model_dir_exp = os.path.expanduser(args.model_dir)\n-            saver = tf.train.import_meta_graph(os.path.join(model_dir_exp, meta_file), clear_devices=True)\n-            tf.get_default_session().run(tf.global_variables_initializer())\n-            tf.get_default_session().run(tf.local_variables_initializer())\n-            saver.restore(tf.get_default_session(), os.path.join(model_dir_exp, ckpt_file))\n-            \n-            # Retrieve the protobuf graph definition and fix the batch norm nodes\n-            input_graph_def = sess.graph.as_graph_def()\n-            \n-            # Freeze the graph def\n-            output_graph_def = freeze_graph_def(sess, input_graph_def, \'embeddings,label_batch\')\n-\n-        # Serialize and dump the output graph to the filesystem\n-        with tf.gfile.GFile(args.output_file, \'wb\') as f:\n-            f.write(output_graph_def.SerializeToString())\n-        print("%d ops in the final graph: %s" % (len(output_graph_def.node), args.output_file))\n-        \n-def freeze_graph_def(sess, input_graph_def, output_node_names):\n-    for node in input_graph_def.node:\n-        if node.op == \'RefSwitch\':\n-            node.op = \'Switch\'\n-            for index in xrange(len(node.input)):\n-                if \'moving_\' in node.input[index]:\n-                    node.input[index] = node.input[index] + \'/read\'\n-        elif node.op == \'AssignSub\':\n-            node.op = \'Sub\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-        elif node.op == \'AssignAdd\':\n-            node.op = \'Add\'\n-            if \'use_locking\' in node.attr: del node.attr[\'use_locking\']\n-    \n-    # Get the list of important nodes\n-    whitelist_names = []\n-    for node in input_graph_def.node:\n-        if (node.name.startswith(\'InceptionResnet\') or node.name.startswith(\'embeddings\') or \n-                node.name.startswith(\'image_batch\') or node.name.startswith(\'label_batch\') or\n-                node.name.startswith(\'phase_train\') or node.name.startswith(\'Logits\')):\n-            whitelist_names.append(node.name)\n-\n-    # Replace all the variables in the graph with constants of the same values\n-    output_graph_def = graph_util.convert_variables_to_constants(\n-        sess, input_graph_def, output_node_names.split(","),\n-        variable_names_whitelist=whitelist_names)\n-    return output_graph_def\n-  \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'model_dir\', type=str, \n-        help=\'Directory containing the metagraph (.meta) file and the checkpoint (ckpt) file containing model parameters\')\n-    parser.add_argument(\'output_file\', type=str, \n-        help=\'Filename for the exported graphdef protobuf (.pb)\')\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/lfw.py b/src/lfw.py\ndeleted file mode 100644\nindex 9194433..0000000\n--- a/src/lfw.py\n+++ /dev/null\n@@ -1,86 +0,0 @@\n-"""Helper for evaluation on the Labeled Faces in the Wild dataset \n-"""\n-\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import os\n-import numpy as np\n-import facenet\n-\n-def evaluate(embeddings, actual_issame, nrof_folds=10, distance_metric=0, subtract_mean=False):\n-    # Calculate evaluation metrics\n-    thresholds = np.arange(0, 4, 0.01)\n-    embeddings1 = embeddings[0::2]\n-    embeddings2 = embeddings[1::2]\n-    tpr, fpr, accuracy = facenet.calculate_roc(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    thresholds = np.arange(0, 4, 0.001)\n-    val, val_std, far = facenet.calculate_val(thresholds, embeddings1, embeddings2,\n-        np.asarray(actual_issame), 1e-3, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    return tpr, fpr, accuracy, val, val_std, far\n-\n-def get_paths(lfw_dir, pairs):\n-    nrof_skipped_pairs = 0\n-    path_list = []\n-    issame_list = []\n-    for pair in pairs:\n-        if len(pair) == 3:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[2])))\n-            issame = True\n-        elif len(pair) == 4:\n-            path0 = add_extension(os.path.join(lfw_dir, pair[0], pair[0] + \'_\' + \'%04d\' % int(pair[1])))\n-            path1 = add_extension(os.path.join(lfw_dir, pair[2], pair[2] + \'_\' + \'%04d\' % int(pair[3])))\n-            issame = False\n-        if os.path.exists(path0) and os.path.exists(path1):    # Only add the pair if both paths exist\n-            path_list += (path0,path1)\n-            issame_list.append(issame)\n-        else:\n-            nrof_skipped_pairs += 1\n-    if nrof_skipped_pairs>0:\n-        print(\'Skipped %d image pairs\' % nrof_skipped_pairs)\n-    \n-    return path_list, issame_list\n-  \n-def add_extension(path):\n-    if os.path.exists(path+\'.jpg\'):\n-        return path+\'.jpg\'\n-    elif os.path.exists(path+\'.png\'):\n-        return path+\'.png\'\n-    else:\n-        raise RuntimeError(\'No file "%s" with extension png or jpg.\' % path)\n-\n-def read_pairs(pairs_filename):\n-    pairs = []\n-    with open(pairs_filename, \'r\') as f:\n-        for line in f.readlines()[1:]:\n-            pair = line.strip().split()\n-            pairs.append(pair)\n-    return np.array(pairs)\n-\n-\n-\ndiff --git a/src/train_softmax.py b/src/train_softmax.py\ndeleted file mode 100644\nindex 6b0b28b..0000000\n--- a/src/train_softmax.py\n+++ /dev/null\n@@ -1,580 +0,0 @@\n-"""Training a face recognizer with TensorFlow using softmax cross entropy loss\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from datetime import datetime\n-import os.path\n-import time\n-import sys\n-import random\n-import tensorflow as tf\n-import numpy as np\n-import importlib\n-import argparse\n-import facenet\n-import lfw\n-import h5py\n-import math\n-import tensorflow.contrib.slim as slim\n-from tensorflow.python.ops import data_flow_ops\n-from tensorflow.python.framework import ops\n-from tensorflow.python.ops import array_ops\n-\n-def main(args):\n-  \n-    network = importlib.import_module(args.model_def)\n-    image_size = (args.image_size, args.image_size)\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n-        os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-\n-    stat_file_name = os.path.join(log_dir, \'stat.h5\')\n-\n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n-\n-    np.random.seed(seed=args.seed)\n-    random.seed(args.seed)\n-    dataset = facenet.get_dataset(args.data_dir)\n-    if args.filter_filename:\n-        dataset = filter_dataset(dataset, os.path.expanduser(args.filter_filename), \n-            args.filter_percentile, args.filter_min_nrof_images_per_class)\n-        \n-    if args.validation_set_split_ratio>0.0:\n-        train_set, val_set = facenet.split_dataset(dataset, args.validation_set_split_ratio, args.min_nrof_val_images_per_class, \'SPLIT_IMAGES\')\n-    else:\n-        train_set, val_set = dataset, []\n-        \n-    nrof_classes = len(train_set)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n-    pretrained_model = None\n-    if args.pretrained_model:\n-        pretrained_model = os.path.expanduser(args.pretrained_model)\n-        print(\'Pre-trained model: %s\' % pretrained_model)\n-    \n-    if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n-        # Read the file containing the pairs used for testing\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-        # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-        \n-        # Get a list of image paths and their labels\n-        image_list, label_list = facenet.get_image_paths_and_labels(train_set)\n-        assert len(image_list)>0, \'The training set should not be empty\'\n-        \n-        val_image_list, val_label_list = facenet.get_image_paths_and_labels(val_set)\n-\n-        # Create a queue that produces indices into the image_list and label_list \n-        labels = ops.convert_to_tensor(label_list, dtype=tf.int32)\n-        range_size = array_ops.shape(labels)[0]\n-        index_queue = tf.train.range_input_producer(range_size, num_epochs=None,\n-                             shuffle=True, seed=None, capacity=32)\n-        \n-        index_dequeue_op = index_queue.dequeue_many(args.batch_size*args.epoch_size, \'index_dequeue\')\n-        \n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-        control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-        \n-        nrof_preprocess_threads = 4\n-        input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                    dtypes=[tf.string, tf.int32, tf.int32],\n-                                    shapes=[(1,), (1,), (1,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'enqueue_op\')\n-        image_batch, label_batch = facenet.create_input_pipeline(input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        label_batch = tf.identity(label_batch, \'label_batch\')\n-        \n-        print(\'Number of classes in training set: %d\' % nrof_classes)\n-        print(\'Number of examples in training set: %d\' % len(image_list))\n-\n-        print(\'Number of classes in validation set: %d\' % len(val_set))\n-        print(\'Number of examples in validation set: %d\' % len(val_image_list))\n-        \n-        print(\'Building training graph\')\n-        \n-        # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size, \n-            weight_decay=args.weight_decay)\n-        logits = slim.fully_connected(prelogits, len(train_set), activation_fn=None, \n-                weights_initializer=slim.initializers.xavier_initializer(), \n-                weights_regularizer=slim.l2_regularizer(args.weight_decay),\n-                scope=\'Logits\', reuse=False)\n-\n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n-\n-        # Norm for the prelogits\n-        eps = 1e-4\n-        prelogits_norm = tf.reduce_mean(tf.norm(tf.abs(prelogits)+eps, ord=args.prelogits_norm_p, axis=1))\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_norm * args.prelogits_norm_loss_factor)\n-\n-        # Add center loss\n-        prelogits_center_loss, _ = facenet.center_loss(prelogits, label_batch, args.center_loss_alfa, nrof_classes)\n-        tf.add_to_collection(tf.GraphKeys.REGULARIZATION_LOSSES, prelogits_center_loss * args.center_loss_factor)\n-\n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n-\n-        # Calculate the average cross entropy loss across the batch\n-        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n-            labels=label_batch, logits=logits, name=\'cross_entropy_per_example\')\n-        cross_entropy_mean = tf.reduce_mean(cross_entropy, name=\'cross_entropy\')\n-        tf.add_to_collection(\'losses\', cross_entropy_mean)\n-        \n-        correct_prediction = tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(label_batch, tf.int64)), tf.float32)\n-        accuracy = tf.reduce_mean(correct_prediction)\n-        \n-        # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([cross_entropy_mean] + regularization_losses, name=\'total_loss\')\n-\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables(), args.log_histograms)\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-\n-        # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n-\n-        # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\n-        sess.run(tf.global_variables_initializer())\n-        sess.run(tf.local_variables_initializer())\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-\n-            if pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % pretrained_model)\n-                saver.restore(sess, pretrained_model)\n-\n-            # Training and validation loop\n-            print(\'Running training\')\n-            nrof_steps = args.max_nrof_epochs*args.epoch_size\n-            nrof_val_samples = int(math.ceil(args.max_nrof_epochs / args.validate_every_n_epochs))   # Validate every validate_every_n_epochs as well as in the last epoch\n-            stat = {\n-                \'loss\': np.zeros((nrof_steps,), np.float32),\n-                \'center_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'reg_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'xent_loss\': np.zeros((nrof_steps,), np.float32),\n-                \'prelogits_norm\': np.zeros((nrof_steps,), np.float32),\n-                \'accuracy\': np.zeros((nrof_steps,), np.float32),\n-                \'val_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_xent_loss\': np.zeros((nrof_val_samples,), np.float32),\n-                \'val_accuracy\': np.zeros((nrof_val_samples,), np.float32),\n-                \'lfw_accuracy\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'lfw_valrate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'learning_rate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_train\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_validate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'time_evaluate\': np.zeros((args.max_nrof_epochs,), np.float32),\n-                \'prelogits_hist\': np.zeros((args.max_nrof_epochs, 1000), np.float32),\n-              }\n-            for epoch in range(1,args.max_nrof_epochs+1):\n-                step = sess.run(global_step, feed_dict=None)\n-                # Train for one epoch\n-                t = time.time()\n-                cont = train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder,\n-                    learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, global_step, \n-                    total_loss, train_op, summary_op, summary_writer, regularization_losses, args.learning_rate_schedule_file,\n-                    stat, cross_entropy_mean, accuracy, learning_rate,\n-                    prelogits, prelogits_center_loss, args.random_rotate, args.random_crop, args.random_flip, prelogits_norm, args.prelogits_hist_max, args.use_fixed_image_standardization)\n-                stat[\'time_train\'][epoch-1] = time.time() - t\n-                \n-                if not cont:\n-                    break\n-                  \n-                t = time.time()\n-                if len(val_image_list)>0 and ((epoch-1) % args.validate_every_n_epochs == args.validate_every_n_epochs-1 or epoch==args.max_nrof_epochs):\n-                    validate(args, sess, epoch, val_image_list, val_label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-                        phase_train_placeholder, batch_size_placeholder, \n-                        stat, total_loss, regularization_losses, cross_entropy_mean, accuracy, args.validate_every_n_epochs, args.use_fixed_image_standardization)\n-                stat[\'time_validate\'][epoch-1] = time.time() - t\n-\n-                # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, epoch)\n-\n-                # Evaluate on LFW\n-                t = time.time()\n-                if args.lfw_dir:\n-                    evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-                        embeddings, label_batch, lfw_paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, log_dir, step, summary_writer, stat, epoch, \n-                        args.lfw_distance_metric, args.lfw_subtract_mean, args.lfw_use_flipped_images, args.use_fixed_image_standardization)\n-                stat[\'time_evaluate\'][epoch-1] = time.time() - t\n-\n-                print(\'Saving statistics\')\n-                with h5py.File(stat_file_name, \'w\') as f:\n-                    for key, value in stat.iteritems():\n-                        f.create_dataset(key, data=value)\n-    \n-    return model_dir\n-  \n-def find_threshold(var, percentile):\n-    hist, bin_edges = np.histogram(var, 100)\n-    cdf = np.float32(np.cumsum(hist)) / np.sum(hist)\n-    bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n-    #plt.plot(bin_centers, cdf)\n-    threshold = np.interp(percentile*0.01, cdf, bin_centers)\n-    return threshold\n-  \n-def filter_dataset(dataset, data_filename, percentile, min_nrof_images_per_class):\n-    with h5py.File(data_filename,\'r\') as f:\n-        distance_to_center = np.array(f.get(\'distance_to_center\'))\n-        label_list = np.array(f.get(\'label_list\'))\n-        image_list = np.array(f.get(\'image_list\'))\n-        distance_to_center_threshold = find_threshold(distance_to_center, percentile)\n-        indices = np.where(distance_to_center>=distance_to_center_threshold)[0]\n-        filtered_dataset = dataset\n-        removelist = []\n-        for i in indices:\n-            label = label_list[i]\n-            image = image_list[i]\n-            if image in filtered_dataset[label].image_paths:\n-                filtered_dataset[label].image_paths.remove(image)\n-            if len(filtered_dataset[label].image_paths)<min_nrof_images_per_class:\n-                removelist.append(label)\n-\n-        ix = sorted(list(set(removelist)), reverse=True)\n-        for i in ix:\n-            del(filtered_dataset[i])\n-\n-    return filtered_dataset\n-  \n-def train(args, sess, epoch, image_list, label_list, index_dequeue_op, enqueue_op, image_paths_placeholder, labels_placeholder, \n-      learning_rate_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, step, \n-      loss, train_op, summary_op, summary_writer, reg_losses, learning_rate_schedule_file, \n-      stat, cross_entropy_mean, accuracy, \n-      learning_rate, prelogits, prelogits_center_loss, random_rotate, random_crop, random_flip, prelogits_norm, prelogits_hist_max, use_fixed_image_standardization):\n-    batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n-        lr = args.learning_rate\n-    else:\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-        \n-    if lr<=0:\n-        return False \n-\n-    index_epoch = sess.run(index_dequeue_op)\n-    label_epoch = np.array(label_list)[index_epoch]\n-    image_epoch = np.array(image_list)[index_epoch]\n-    \n-    # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_epoch),1)\n-    image_paths_array = np.expand_dims(np.array(image_epoch),1)\n-    control_value = facenet.RANDOM_ROTATE * random_rotate + facenet.RANDOM_CROP * random_crop + facenet.RANDOM_FLIP * random_flip + facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n-    control_array = np.ones_like(labels_array) * control_value\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-\n-    # Training loop\n-    train_time = 0\n-    while batch_number < args.epoch_size:\n-        start_time = time.time()\n-        feed_dict = {learning_rate_placeholder: lr, phase_train_placeholder:True, batch_size_placeholder:args.batch_size}\n-        tensor_list = [loss, train_op, step, reg_losses, prelogits, cross_entropy_mean, learning_rate, prelogits_norm, accuracy, prelogits_center_loss]\n-        if batch_number % 100 == 0:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_, summary_str = sess.run(tensor_list + [summary_op], feed_dict=feed_dict)\n-            summary_writer.add_summary(summary_str, global_step=step_)\n-        else:\n-            loss_, _, step_, reg_losses_, prelogits_, cross_entropy_mean_, lr_, prelogits_norm_, accuracy_, center_loss_ = sess.run(tensor_list, feed_dict=feed_dict)\n-         \n-        duration = time.time() - start_time\n-        stat[\'loss\'][step_-1] = loss_\n-        stat[\'center_loss\'][step_-1] = center_loss_\n-        stat[\'reg_loss\'][step_-1] = np.sum(reg_losses_)\n-        stat[\'xent_loss\'][step_-1] = cross_entropy_mean_\n-        stat[\'prelogits_norm\'][step_-1] = prelogits_norm_\n-        stat[\'learning_rate\'][epoch-1] = lr_\n-        stat[\'accuracy\'][step_-1] = accuracy_\n-        stat[\'prelogits_hist\'][epoch-1,:] += np.histogram(np.minimum(np.abs(prelogits_), prelogits_hist_max), bins=1000, range=(0.0, prelogits_hist_max))[0]\n-        \n-        duration = time.time() - start_time\n-        print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tRegLoss %2.3f\\tAccuracy %2.3f\\tLr %2.5f\\tCl %2.3f\' %\n-              (epoch, batch_number+1, args.epoch_size, duration, loss_, cross_entropy_mean_, np.sum(reg_losses_), accuracy_, lr_, center_loss_))\n-        batch_number += 1\n-        train_time += duration\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/total\', simple_value=train_time)\n-    summary_writer.add_summary(summary, global_step=step_)\n-    return True\n-\n-def validate(args, sess, epoch, image_list, label_list, enqueue_op, image_paths_placeholder, labels_placeholder, control_placeholder,\n-             phase_train_placeholder, batch_size_placeholder, \n-             stat, loss, regularization_losses, cross_entropy_mean, accuracy, validate_every_n_epochs, use_fixed_image_standardization):\n-  \n-    print(\'Running forward pass on validation set\')\n-\n-    nrof_batches = len(label_list) // args.lfw_batch_size\n-    nrof_images = nrof_batches * args.lfw_batch_size\n-    \n-    # Enqueue one epoch of image paths and labels\n-    labels_array = np.expand_dims(np.array(label_list[:nrof_images]),1)\n-    image_paths_array = np.expand_dims(np.array(image_list[:nrof_images]),1)\n-    control_array = np.ones_like(labels_array, np.int32)*facenet.FIXED_STANDARDIZATION * use_fixed_image_standardization\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-\n-    loss_array = np.zeros((nrof_batches,), np.float32)\n-    xent_array = np.zeros((nrof_batches,), np.float32)\n-    accuracy_array = np.zeros((nrof_batches,), np.float32)\n-\n-    # Training loop\n-    start_time = time.time()\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:args.lfw_batch_size}\n-        loss_, cross_entropy_mean_, accuracy_ = sess.run([loss, cross_entropy_mean, accuracy], feed_dict=feed_dict)\n-        loss_array[i], xent_array[i], accuracy_array[i] = (loss_, cross_entropy_mean_, accuracy_)\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-\n-    duration = time.time() - start_time\n-\n-    val_index = (epoch-1)//validate_every_n_epochs\n-    stat[\'val_loss\'][val_index] = np.mean(loss_array)\n-    stat[\'val_xent_loss\'][val_index] = np.mean(xent_array)\n-    stat[\'val_accuracy\'][val_index] = np.mean(accuracy_array)\n-\n-    print(\'Validation Epoch: %d\\tTime %.3f\\tLoss %2.3f\\tXent %2.3f\\tAccuracy %2.3f\' %\n-          (epoch, duration, np.mean(loss_array), np.mean(xent_array), np.mean(accuracy_array)))\n-\n-\n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder, \n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, log_dir, step, summary_writer, stat, epoch, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n-    start_time = time.time()\n-    # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n-    # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n-    nrof_flips = 2 if use_flipped_images else 1\n-    nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n-    control_array = np.zeros_like(labels_array, np.int32)\n-    if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n-    if use_flipped_images:\n-        # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n-    embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n-    nrof_batches = nrof_images // batch_size\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    lab_array = np.zeros((nrof_images,))\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n-        lab_array[lab] = lab\n-        emb_array[lab, :] = emb\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n-    if use_flipped_images:\n-        # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n-    else:\n-        embeddings = emb_array\n-\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    _, _, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    lfw_time = time.time() - start_time\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n-    summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n-    stat[\'lfw_accuracy\'][epoch-1] = np.mean(accuracy)\n-    stat[\'lfw_valrate\'][epoch-1] = val\n-\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n-    # Save the model checkpoint\n-    print(\'Saving variables\')\n-    start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-    save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n-    if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n-        start_time = time.time()\n-        saver.export_meta_graph(metagraph_filename)\n-        save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n-    summary_writer.add_summary(summary, step)\n-  \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--random_rotate\', \n-        help=\'Performs random rotations of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--center_loss_factor\', type=float,\n-        help=\'Center loss factor.\', default=0.0)\n-    parser.add_argument(\'--center_loss_alfa\', type=float,\n-        help=\'Center update rate for center loss.\', default=0.95)\n-    parser.add_argument(\'--prelogits_norm_loss_factor\', type=float,\n-        help=\'Loss based on the norm of the activations in the prelogits layer.\', default=0.0)\n-    parser.add_argument(\'--prelogits_norm_p\', type=float,\n-        help=\'Norm to use for prelogits norm loss.\', default=1.0)\n-    parser.add_argument(\'--prelogits_hist_max\', type=float,\n-        help=\'The max value for the prelogits histogram.\', default=10.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--nrof_preprocess_threads\', type=int,\n-        help=\'Number of preprocessing (data loading and augmentation) threads.\', default=4)\n-    parser.add_argument(\'--log_histograms\', \n-        help=\'Enables logging of weight/bias histograms in tensorboard.\', action=\'store_true\')\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-    parser.add_argument(\'--filter_filename\', type=str,\n-        help=\'File containing image data used for dataset filtering\', default=\'\')\n-    parser.add_argument(\'--filter_percentile\', type=float,\n-        help=\'Keep only the percentile images closed to its class center\', default=100.0)\n-    parser.add_argument(\'--filter_min_nrof_images_per_class\', type=int,\n-        help=\'Keep only the classes with this number of examples or more\', default=0)\n-    parser.add_argument(\'--validate_every_n_epochs\', type=int,\n-        help=\'Number of epoch between validation\', default=5)\n-    parser.add_argument(\'--validation_set_split_ratio\', type=float,\n-        help=\'The ratio of the total dataset to use for validation\', default=0.0)\n-    parser.add_argument(\'--min_nrof_val_images_per_class\', type=float,\n-        help=\'Classes with fewer images will be removed from the validation set\', default=0)\n- \n-    # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--lfw_distance_metric\', type=int,\n-        help=\'Type of distance metric to use. 0: Euclidian, 1:Cosine similarity distance.\', default=0)\n-    parser.add_argument(\'--lfw_use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--lfw_subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n-    return parser.parse_args(argv)\n-  \n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/train_tripletloss.py b/src/train_tripletloss.py\ndeleted file mode 100644\nindex d6df19a..0000000\n--- a/src/train_tripletloss.py\n+++ /dev/null\n@@ -1,486 +0,0 @@\n-"""Training a face recognizer with TensorFlow based on the FaceNet paper\n-FaceNet: A Unified Embedding for Face Recognition and Clustering: http://arxiv.org/abs/1503.03832\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-from datetime import datetime\n-import os.path\n-import time\n-import sys\n-import tensorflow as tf\n-import numpy as np\n-import importlib\n-import itertools\n-import argparse\n-import facenet\n-import lfw\n-\n-from tensorflow.python.ops import data_flow_ops\n-\n-from six.moves import xrange  # @UnresolvedImport\n-\n-def main(args):\n-  \n-    network = importlib.import_module(args.model_def)\n-\n-    subdir = datetime.strftime(datetime.now(), \'%Y%m%d-%H%M%S\')\n-    log_dir = os.path.join(os.path.expanduser(args.logs_base_dir), subdir)\n-    if not os.path.isdir(log_dir):  # Create the log directory if it doesn\'t exist\n-        os.makedirs(log_dir)\n-    model_dir = os.path.join(os.path.expanduser(args.models_base_dir), subdir)\n-    if not os.path.isdir(model_dir):  # Create the model directory if it doesn\'t exist\n-        os.makedirs(model_dir)\n-\n-    # Write arguments to a text file\n-    facenet.write_arguments_to_file(args, os.path.join(log_dir, \'arguments.txt\'))\n-        \n-    # Store some git revision info in a text file in the log directory\n-    src_path,_ = os.path.split(os.path.realpath(__file__))\n-    facenet.store_revision_info(src_path, log_dir, \' \'.join(sys.argv))\n-\n-    np.random.seed(seed=args.seed)\n-    train_set = facenet.get_dataset(args.data_dir)\n-    \n-    print(\'Model directory: %s\' % model_dir)\n-    print(\'Log directory: %s\' % log_dir)\n-    if args.pretrained_model:\n-        print(\'Pre-trained model: %s\' % os.path.expanduser(args.pretrained_model))\n-    \n-    if args.lfw_dir:\n-        print(\'LFW directory: %s\' % args.lfw_dir)\n-        # Read the file containing the pairs used for testing\n-        pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-        # Get the paths for the corresponding images\n-        lfw_paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-        \n-    \n-    with tf.Graph().as_default():\n-        tf.set_random_seed(args.seed)\n-        global_step = tf.Variable(0, trainable=False)\n-\n-        # Placeholder for the learning rate\n-        learning_rate_placeholder = tf.placeholder(tf.float32, name=\'learning_rate\')\n-        \n-        batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-        \n-        phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n-        \n-        image_paths_placeholder = tf.placeholder(tf.string, shape=(None,3), name=\'image_paths\')\n-        labels_placeholder = tf.placeholder(tf.int64, shape=(None,3), name=\'labels\')\n-        \n-        input_queue = data_flow_ops.FIFOQueue(capacity=100000,\n-                                    dtypes=[tf.string, tf.int64],\n-                                    shapes=[(3,), (3,)],\n-                                    shared_name=None, name=None)\n-        enqueue_op = input_queue.enqueue_many([image_paths_placeholder, labels_placeholder])\n-        \n-        nrof_preprocess_threads = 4\n-        images_and_labels = []\n-        for _ in range(nrof_preprocess_threads):\n-            filenames, label = input_queue.dequeue()\n-            images = []\n-            for filename in tf.unstack(filenames):\n-                file_contents = tf.read_file(filename)\n-                image = tf.image.decode_image(file_contents, channels=3)\n-                \n-                if args.random_crop:\n-                    image = tf.random_crop(image, [args.image_size, args.image_size, 3])\n-                else:\n-                    image = tf.image.resize_image_with_crop_or_pad(image, args.image_size, args.image_size)\n-                if args.random_flip:\n-                    image = tf.image.random_flip_left_right(image)\n-    \n-                #pylint: disable=no-member\n-                image.set_shape((args.image_size, args.image_size, 3))\n-                images.append(tf.image.per_image_standardization(image))\n-            images_and_labels.append([images, label])\n-    \n-        image_batch, labels_batch = tf.train.batch_join(\n-            images_and_labels, batch_size=batch_size_placeholder, \n-            shapes=[(args.image_size, args.image_size, 3), ()], enqueue_many=True,\n-            capacity=4 * nrof_preprocess_threads * args.batch_size,\n-            allow_smaller_final_batch=True)\n-        image_batch = tf.identity(image_batch, \'image_batch\')\n-        image_batch = tf.identity(image_batch, \'input\')\n-        labels_batch = tf.identity(labels_batch, \'label_batch\')\n-\n-        # Build the inference graph\n-        prelogits, _ = network.inference(image_batch, args.keep_probability, \n-            phase_train=phase_train_placeholder, bottleneck_layer_size=args.embedding_size,\n-            weight_decay=args.weight_decay)\n-        \n-        embeddings = tf.nn.l2_normalize(prelogits, 1, 1e-10, name=\'embeddings\')\n-        # Split embeddings into anchor, positive and negative and calculate triplet loss\n-        anchor, positive, negative = tf.unstack(tf.reshape(embeddings, [-1,3,args.embedding_size]), 3, 1)\n-        triplet_loss = facenet.triplet_loss(anchor, positive, negative, args.alpha)\n-        \n-        learning_rate = tf.train.exponential_decay(learning_rate_placeholder, global_step,\n-            args.learning_rate_decay_epochs*args.epoch_size, args.learning_rate_decay_factor, staircase=True)\n-        tf.summary.scalar(\'learning_rate\', learning_rate)\n-\n-        # Calculate the total losses\n-        regularization_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n-        total_loss = tf.add_n([triplet_loss] + regularization_losses, name=\'total_loss\')\n-\n-        # Build a Graph that trains the model with one batch of examples and updates the model parameters\n-        train_op = facenet.train(total_loss, global_step, args.optimizer, \n-            learning_rate, args.moving_average_decay, tf.global_variables())\n-        \n-        # Create a saver\n-        saver = tf.train.Saver(tf.trainable_variables(), max_to_keep=3)\n-\n-        # Build the summary operation based on the TF collection of Summaries.\n-        summary_op = tf.summary.merge_all()\n-\n-        # Start running operations on the Graph.\n-        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_memory_fraction)\n-        sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))        \n-\n-        # Initialize variables\n-        sess.run(tf.global_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-        sess.run(tf.local_variables_initializer(), feed_dict={phase_train_placeholder:True})\n-\n-        summary_writer = tf.summary.FileWriter(log_dir, sess.graph)\n-        coord = tf.train.Coordinator()\n-        tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-        with sess.as_default():\n-\n-            if args.pretrained_model:\n-                print(\'Restoring pretrained model: %s\' % args.pretrained_model)\n-                saver.restore(sess, os.path.expanduser(args.pretrained_model))\n-\n-            # Training and validation loop\n-            epoch = 0\n-            while epoch < args.max_nrof_epochs:\n-                step = sess.run(global_step, feed_dict=None)\n-                epoch = step // args.epoch_size\n-                # Train for one epoch\n-                train(args, sess, train_set, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-                    batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-                    embeddings, total_loss, train_op, summary_op, summary_writer, args.learning_rate_schedule_file,\n-                    args.embedding_size, anchor, positive, negative, triplet_loss)\n-\n-                # Save variables and the metagraph if it doesn\'t exist already\n-                save_variables_and_metagraph(sess, saver, summary_writer, model_dir, subdir, step)\n-\n-                # Evaluate on LFW\n-                if args.lfw_dir:\n-                    evaluate(sess, lfw_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-                            batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, args.batch_size, \n-                            args.lfw_nrof_folds, log_dir, step, summary_writer, args.embedding_size)\n-\n-    return model_dir\n-\n-\n-def train(args, sess, dataset, epoch, image_paths_placeholder, labels_placeholder, labels_batch,\n-          batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, input_queue, global_step, \n-          embeddings, loss, train_op, summary_op, summary_writer, learning_rate_schedule_file,\n-          embedding_size, anchor, positive, negative, triplet_loss):\n-    batch_number = 0\n-    \n-    if args.learning_rate>0.0:\n-        lr = args.learning_rate\n-    else:\n-        lr = facenet.get_learning_rate_from_file(learning_rate_schedule_file, epoch)\n-    while batch_number < args.epoch_size:\n-        # Sample people randomly from the dataset\n-        image_paths, num_per_class = sample_people(dataset, args.people_per_batch, args.images_per_person)\n-        \n-        print(\'Running forward pass on sampled images: \', end=\'\')\n-        start_time = time.time()\n-        nrof_examples = args.people_per_batch * args.images_per_person\n-        labels_array = np.reshape(np.arange(nrof_examples),(-1,3))\n-        image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        nrof_batches = int(np.ceil(nrof_examples / args.batch_size))\n-        for i in range(nrof_batches):\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size, \n-                learning_rate_placeholder: lr, phase_train_placeholder: True})\n-            emb_array[lab,:] = emb\n-        print(\'%.3f\' % (time.time()-start_time))\n-\n-        # Select triplets based on the embeddings\n-        print(\'Selecting suitable triplets for training\')\n-        triplets, nrof_random_negs, nrof_triplets = select_triplets(emb_array, num_per_class, \n-            image_paths, args.people_per_batch, args.alpha)\n-        selection_time = time.time() - start_time\n-        print(\'(nrof_random_negs, nrof_triplets) = (%d, %d): time=%.3f seconds\' % \n-            (nrof_random_negs, nrof_triplets, selection_time))\n-\n-        # Perform training on the selected triplets\n-        nrof_batches = int(np.ceil(nrof_triplets*3/args.batch_size))\n-        triplet_paths = list(itertools.chain(*triplets))\n-        labels_array = np.reshape(np.arange(len(triplet_paths)),(-1,3))\n-        triplet_paths_array = np.reshape(np.expand_dims(np.array(triplet_paths),1), (-1,3))\n-        sess.run(enqueue_op, {image_paths_placeholder: triplet_paths_array, labels_placeholder: labels_array})\n-        nrof_examples = len(triplet_paths)\n-        train_time = 0\n-        i = 0\n-        emb_array = np.zeros((nrof_examples, embedding_size))\n-        loss_array = np.zeros((nrof_triplets,))\n-        summary = tf.Summary()\n-        step = 0\n-        while i < nrof_batches:\n-            start_time = time.time()\n-            batch_size = min(nrof_examples-i*args.batch_size, args.batch_size)\n-            feed_dict = {batch_size_placeholder: batch_size, learning_rate_placeholder: lr, phase_train_placeholder: True}\n-            err, _, step, emb, lab = sess.run([loss, train_op, global_step, embeddings, labels_batch], feed_dict=feed_dict)\n-            emb_array[lab,:] = emb\n-            loss_array[i] = err\n-            duration = time.time() - start_time\n-            print(\'Epoch: [%d][%d/%d]\\tTime %.3f\\tLoss %2.3f\' %\n-                  (epoch, batch_number+1, args.epoch_size, duration, err))\n-            batch_number += 1\n-            i += 1\n-            train_time += duration\n-            summary.value.add(tag=\'loss\', simple_value=err)\n-            \n-        # Add validation loss and accuracy to summary\n-        #pylint: disable=maybe-no-member\n-        summary.value.add(tag=\'time/selection\', simple_value=selection_time)\n-        summary_writer.add_summary(summary, step)\n-    return step\n-  \n-def select_triplets(embeddings, nrof_images_per_class, image_paths, people_per_batch, alpha):\n-    """ Select the triplets for training\n-    """\n-    trip_idx = 0\n-    emb_start_idx = 0\n-    num_trips = 0\n-    triplets = []\n-    \n-    # VGG Face: Choosing good triplets is crucial and should strike a balance between\n-    #  selecting informative (i.e. challenging) examples and swamping training with examples that\n-    #  are too hard. This is achieve by extending each pair (a, p) to a triplet (a, p, n) by sampling\n-    #  the image n at random, but only between the ones that violate the triplet loss margin. The\n-    #  latter is a form of hard-negative mining, but it is not as aggressive (and much cheaper) than\n-    #  choosing the maximally violating example, as often done in structured output learning.\n-\n-    for i in xrange(people_per_batch):\n-        nrof_images = int(nrof_images_per_class[i])\n-        for j in xrange(1,nrof_images):\n-            a_idx = emb_start_idx + j - 1\n-            neg_dists_sqr = np.sum(np.square(embeddings[a_idx] - embeddings), 1)\n-            for pair in xrange(j, nrof_images): # For every possible positive pair.\n-                p_idx = emb_start_idx + pair\n-                pos_dist_sqr = np.sum(np.square(embeddings[a_idx]-embeddings[p_idx]))\n-                neg_dists_sqr[emb_start_idx:emb_start_idx+nrof_images] = np.NaN\n-                #all_neg = np.where(np.logical_and(neg_dists_sqr-pos_dist_sqr<alpha, pos_dist_sqr<neg_dists_sqr))[0]  # FaceNet selection\n-                all_neg = np.where(neg_dists_sqr-pos_dist_sqr<alpha)[0] # VGG Face selecction\n-                nrof_random_negs = all_neg.shape[0]\n-                if nrof_random_negs>0:\n-                    rnd_idx = np.random.randint(nrof_random_negs)\n-                    n_idx = all_neg[rnd_idx]\n-                    triplets.append((image_paths[a_idx], image_paths[p_idx], image_paths[n_idx]))\n-                    #print(\'Triplet %d: (%d, %d, %d), pos_dist=%2.6f, neg_dist=%2.6f (%d, %d, %d, %d, %d)\' % \n-                    #    (trip_idx, a_idx, p_idx, n_idx, pos_dist_sqr, neg_dists_sqr[n_idx], nrof_random_negs, rnd_idx, i, j, emb_start_idx))\n-                    trip_idx += 1\n-\n-                num_trips += 1\n-\n-        emb_start_idx += nrof_images\n-\n-    np.random.shuffle(triplets)\n-    return triplets, num_trips, len(triplets)\n-\n-def sample_people(dataset, people_per_batch, images_per_person):\n-    nrof_images = people_per_batch * images_per_person\n-  \n-    # Sample classes from the dataset\n-    nrof_classes = len(dataset)\n-    class_indices = np.arange(nrof_classes)\n-    np.random.shuffle(class_indices)\n-    \n-    i = 0\n-    image_paths = []\n-    num_per_class = []\n-    sampled_class_indices = []\n-    # Sample images from these classes until we have enough\n-    while len(image_paths)<nrof_images:\n-        class_index = class_indices[i]\n-        nrof_images_in_class = len(dataset[class_index])\n-        image_indices = np.arange(nrof_images_in_class)\n-        np.random.shuffle(image_indices)\n-        nrof_images_from_class = min(nrof_images_in_class, images_per_person, nrof_images-len(image_paths))\n-        idx = image_indices[0:nrof_images_from_class]\n-        image_paths_for_class = [dataset[class_index].image_paths[j] for j in idx]\n-        sampled_class_indices += [class_index]*nrof_images_from_class\n-        image_paths += image_paths_for_class\n-        num_per_class.append(nrof_images_from_class)\n-        i+=1\n-  \n-    return image_paths, num_per_class\n-\n-def evaluate(sess, image_paths, embeddings, labels_batch, image_paths_placeholder, labels_placeholder, \n-        batch_size_placeholder, learning_rate_placeholder, phase_train_placeholder, enqueue_op, actual_issame, batch_size, \n-        nrof_folds, log_dir, step, summary_writer, embedding_size):\n-    start_time = time.time()\n-    # Run forward pass to calculate embeddings\n-    print(\'Running forward pass on LFW images: \', end=\'\')\n-    \n-    nrof_images = len(actual_issame)*2\n-    assert(len(image_paths)==nrof_images)\n-    labels_array = np.reshape(np.arange(nrof_images),(-1,3))\n-    image_paths_array = np.reshape(np.expand_dims(np.array(image_paths),1), (-1,3))\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array})\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    nrof_batches = int(np.ceil(nrof_images / batch_size))\n-    label_check_array = np.zeros((nrof_images,))\n-    for i in xrange(nrof_batches):\n-        batch_size = min(nrof_images-i*batch_size, batch_size)\n-        emb, lab = sess.run([embeddings, labels_batch], feed_dict={batch_size_placeholder: batch_size,\n-            learning_rate_placeholder: 0.0, phase_train_placeholder: False})\n-        emb_array[lab,:] = emb\n-        label_check_array[lab] = 1\n-    print(\'%.3f\' % (time.time()-start_time))\n-    \n-    assert(np.all(label_check_array==1))\n-    \n-    _, _, accuracy, val, val_std, far = lfw.evaluate(emb_array, actual_issame, nrof_folds=nrof_folds)\n-    \n-    print(\'Accuracy: %1.3f+-%1.3f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    lfw_time = time.time() - start_time\n-    # Add validation loss and accuracy to summary\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'lfw/accuracy\', simple_value=np.mean(accuracy))\n-    summary.value.add(tag=\'lfw/val_rate\', simple_value=val)\n-    summary.value.add(tag=\'time/lfw\', simple_value=lfw_time)\n-    summary_writer.add_summary(summary, step)\n-    with open(os.path.join(log_dir,\'lfw_result.txt\'),\'at\') as f:\n-        f.write(\'%d\\t%.5f\\t%.5f\\n\' % (step, np.mean(accuracy), val))\n-\n-def save_variables_and_metagraph(sess, saver, summary_writer, model_dir, model_name, step):\n-    # Save the model checkpoint\n-    print(\'Saving variables\')\n-    start_time = time.time()\n-    checkpoint_path = os.path.join(model_dir, \'model-%s.ckpt\' % model_name)\n-    saver.save(sess, checkpoint_path, global_step=step, write_meta_graph=False)\n-    save_time_variables = time.time() - start_time\n-    print(\'Variables saved in %.2f seconds\' % save_time_variables)\n-    metagraph_filename = os.path.join(model_dir, \'model-%s.meta\' % model_name)\n-    save_time_metagraph = 0  \n-    if not os.path.exists(metagraph_filename):\n-        print(\'Saving metagraph\')\n-        start_time = time.time()\n-        saver.export_meta_graph(metagraph_filename)\n-        save_time_metagraph = time.time() - start_time\n-        print(\'Metagraph saved in %.2f seconds\' % save_time_metagraph)\n-    summary = tf.Summary()\n-    #pylint: disable=maybe-no-member\n-    summary.value.add(tag=\'time/save_variables\', simple_value=save_time_variables)\n-    summary.value.add(tag=\'time/save_metagraph\', simple_value=save_time_metagraph)\n-    summary_writer.add_summary(summary, step)\n-  \n-  \n-def get_learning_rate_from_file(filename, epoch):\n-    with open(filename, \'r\') as f:\n-        for line in f.readlines():\n-            line = line.split(\'#\', 1)[0]\n-            if line:\n-                par = line.strip().split(\':\')\n-                e = int(par[0])\n-                lr = float(par[1])\n-                if e <= epoch:\n-                    learning_rate = lr\n-                else:\n-                    return learning_rate\n-    \n-\n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'--logs_base_dir\', type=str, \n-        help=\'Directory where to write event logs.\', default=\'~/logs/facenet\')\n-    parser.add_argument(\'--models_base_dir\', type=str,\n-        help=\'Directory where to write trained models and checkpoints.\', default=\'~/models/facenet\')\n-    parser.add_argument(\'--gpu_memory_fraction\', type=float,\n-        help=\'Upper bound on the amount of GPU memory that will be used by the process.\', default=1.0)\n-    parser.add_argument(\'--pretrained_model\', type=str,\n-        help=\'Load a pretrained model before training starts.\')\n-    parser.add_argument(\'--data_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\',\n-        default=\'~/datasets/casia/casia_maxpy_mtcnnalign_182_160\')\n-    parser.add_argument(\'--model_def\', type=str,\n-        help=\'Model definition. Points to a module containing the definition of the inference graph.\', default=\'models.inception_resnet_v1\')\n-    parser.add_argument(\'--max_nrof_epochs\', type=int,\n-        help=\'Number of epochs to run.\', default=500)\n-    parser.add_argument(\'--batch_size\', type=int,\n-        help=\'Number of images to process in a batch.\', default=90)\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--people_per_batch\', type=int,\n-        help=\'Number of people per batch.\', default=45)\n-    parser.add_argument(\'--images_per_person\', type=int,\n-        help=\'Number of images per person.\', default=40)\n-    parser.add_argument(\'--epoch_size\', type=int,\n-        help=\'Number of batches per epoch.\', default=1000)\n-    parser.add_argument(\'--alpha\', type=float,\n-        help=\'Positive to negative triplet distance margin.\', default=0.2)\n-    parser.add_argument(\'--embedding_size\', type=int,\n-        help=\'Dimensionality of the embedding.\', default=128)\n-    parser.add_argument(\'--random_crop\', \n-        help=\'Performs random cropping of training images. If false, the center image_size pixels from the training images are used. \' +\n-         \'If the size of the images in the data directory is equal to image_size no cropping is performed\', action=\'store_true\')\n-    parser.add_argument(\'--random_flip\', \n-        help=\'Performs random horizontal flipping of training images.\', action=\'store_true\')\n-    parser.add_argument(\'--keep_probability\', type=float,\n-        help=\'Keep probability of dropout for the fully connected layer(s).\', default=1.0)\n-    parser.add_argument(\'--weight_decay\', type=float,\n-        help=\'L2 weight regularization.\', default=0.0)\n-    parser.add_argument(\'--optimizer\', type=str, choices=[\'ADAGRAD\', \'ADADELTA\', \'ADAM\', \'RMSPROP\', \'MOM\'],\n-        help=\'The optimization algorithm to use\', default=\'ADAGRAD\')\n-    parser.add_argument(\'--learning_rate\', type=float,\n-        help=\'Initial learning rate. If set to a negative value a learning rate \' +\n-        \'schedule can be specified in the file "learning_rate_schedule.txt"\', default=0.1)\n-    parser.add_argument(\'--learning_rate_decay_epochs\', type=int,\n-        help=\'Number of epochs between learning rate decay.\', default=100)\n-    parser.add_argument(\'--learning_rate_decay_factor\', type=float,\n-        help=\'Learning rate decay factor.\', default=1.0)\n-    parser.add_argument(\'--moving_average_decay\', type=float,\n-        help=\'Exponential decay for tracking of training parameters.\', default=0.9999)\n-    parser.add_argument(\'--seed\', type=int,\n-        help=\'Random seed.\', default=666)\n-    parser.add_argument(\'--learning_rate_schedule_file\', type=str,\n-        help=\'File containing the learning rate schedule that is used when learning_rate is set to to -1.\', default=\'data/learning_rate_schedule.txt\')\n-\n-    # Parameters for validation on LFW\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned face patches.\', default=\'\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    return parser.parse_args(argv)\n-  \n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))\ndiff --git a/src/validate_on_lfw.py b/src/validate_on_lfw.py\ndeleted file mode 100644\nindex ac456c5..0000000\n--- a/src/validate_on_lfw.py\n+++ /dev/null\n@@ -1,164 +0,0 @@\n-"""Validate a face recognizer on the "Labeled Faces in the Wild" dataset (http://vis-www.cs.umass.edu/lfw/).\n-Embeddings are calculated using the pairs from http://vis-www.cs.umass.edu/lfw/pairs.txt and the ROC curve\n-is calculated and plotted. Both the model metagraph and the model parameters need to exist\n-in the same directory, and the metagraph should have the extension \'.meta\'.\n-"""\n-# MIT License\n-# \n-# Copyright (c) 2016 David Sandberg\n-# \n-# Permission is hereby granted, free of charge, to any person obtaining a copy\n-# of this software and associated documentation files (the "Software"), to deal\n-# in the Software without restriction, including without limitation the rights\n-# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n-# copies of the Software, and to permit persons to whom the Software is\n-# furnished to do so, subject to the following conditions:\n-# \n-# The above copyright notice and this permission notice shall be included in all\n-# copies or substantial portions of the Software.\n-# \n-# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n-# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n-# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n-# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n-# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n-# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n-# SOFTWARE.\n-\n-from __future__ import absolute_import\n-from __future__ import division\n-from __future__ import print_function\n-\n-import tensorflow as tf\n-import numpy as np\n-import argparse\n-import facenet\n-import lfw\n-import os\n-import sys\n-from tensorflow.python.ops import data_flow_ops\n-from sklearn import metrics\n-from scipy.optimize import brentq\n-from scipy import interpolate\n-\n-def main(args):\n-  \n-    with tf.Graph().as_default():\n-      \n-        with tf.Session() as sess:\n-            \n-            # Read the file containing the pairs used for testing\n-            pairs = lfw.read_pairs(os.path.expanduser(args.lfw_pairs))\n-\n-            # Get the paths for the corresponding images\n-            paths, actual_issame = lfw.get_paths(os.path.expanduser(args.lfw_dir), pairs)\n-            \n-            image_paths_placeholder = tf.placeholder(tf.string, shape=(None,1), name=\'image_paths\')\n-            labels_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'labels\')\n-            batch_size_placeholder = tf.placeholder(tf.int32, name=\'batch_size\')\n-            control_placeholder = tf.placeholder(tf.int32, shape=(None,1), name=\'control\')\n-            phase_train_placeholder = tf.placeholder(tf.bool, name=\'phase_train\')\n- \n-            nrof_preprocess_threads = 4\n-            image_size = (args.image_size, args.image_size)\n-            eval_input_queue = data_flow_ops.FIFOQueue(capacity=2000000,\n-                                        dtypes=[tf.string, tf.int32, tf.int32],\n-                                        shapes=[(1,), (1,), (1,)],\n-                                        shared_name=None, name=None)\n-            eval_enqueue_op = eval_input_queue.enqueue_many([image_paths_placeholder, labels_placeholder, control_placeholder], name=\'eval_enqueue_op\')\n-            image_batch, label_batch = facenet.create_input_pipeline(eval_input_queue, image_size, nrof_preprocess_threads, batch_size_placeholder)\n-     \n-            # Load the model\n-            input_map = {\'image_batch\': image_batch, \'label_batch\': label_batch, \'phase_train\': phase_train_placeholder}\n-            facenet.load_model(args.model, input_map=input_map)\n-\n-            # Get output tensor\n-            embeddings = tf.get_default_graph().get_tensor_by_name("embeddings:0")\n-#              \n-            coord = tf.train.Coordinator()\n-            tf.train.start_queue_runners(coord=coord, sess=sess)\n-\n-            evaluate(sess, eval_enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-                embeddings, label_batch, paths, actual_issame, args.lfw_batch_size, args.lfw_nrof_folds, args.distance_metric, args.subtract_mean,\n-                args.use_flipped_images, args.use_fixed_image_standardization)\n-\n-              \n-def evaluate(sess, enqueue_op, image_paths_placeholder, labels_placeholder, phase_train_placeholder, batch_size_placeholder, control_placeholder,\n-        embeddings, labels, image_paths, actual_issame, batch_size, nrof_folds, distance_metric, subtract_mean, use_flipped_images, use_fixed_image_standardization):\n-    # Run forward pass to calculate embeddings\n-    print(\'Runnning forward pass on LFW images\')\n-    \n-    # Enqueue one epoch of image paths and labels\n-    nrof_embeddings = len(actual_issame)*2  # nrof_pairs * nrof_images_per_pair\n-    nrof_flips = 2 if use_flipped_images else 1\n-    nrof_images = nrof_embeddings * nrof_flips\n-    labels_array = np.expand_dims(np.arange(0,nrof_images),1)\n-    image_paths_array = np.expand_dims(np.repeat(np.array(image_paths),nrof_flips),1)\n-    control_array = np.zeros_like(labels_array, np.int32)\n-    if use_fixed_image_standardization:\n-        control_array += np.ones_like(labels_array)*facenet.FIXED_STANDARDIZATION\n-    if use_flipped_images:\n-        # Flip every second image\n-        control_array += (labels_array % 2)*facenet.FLIP\n-    sess.run(enqueue_op, {image_paths_placeholder: image_paths_array, labels_placeholder: labels_array, control_placeholder: control_array})\n-    \n-    embedding_size = int(embeddings.get_shape()[1])\n-    assert nrof_images % batch_size == 0, \'The number of LFW images must be an integer multiple of the LFW batch size\'\n-    nrof_batches = nrof_images // batch_size\n-    emb_array = np.zeros((nrof_images, embedding_size))\n-    lab_array = np.zeros((nrof_images,))\n-    for i in range(nrof_batches):\n-        feed_dict = {phase_train_placeholder:False, batch_size_placeholder:batch_size}\n-        emb, lab = sess.run([embeddings, labels], feed_dict=feed_dict)\n-        lab_array[lab] = lab\n-        emb_array[lab, :] = emb\n-        if i % 10 == 9:\n-            print(\'.\', end=\'\')\n-            sys.stdout.flush()\n-    print(\'\')\n-    embeddings = np.zeros((nrof_embeddings, embedding_size*nrof_flips))\n-    if use_flipped_images:\n-        # Concatenate embeddings for flipped and non flipped version of the images\n-        embeddings[:,:embedding_size] = emb_array[0::2,:]\n-        embeddings[:,embedding_size:] = emb_array[1::2,:]\n-    else:\n-        embeddings = emb_array\n-\n-    assert np.array_equal(lab_array, np.arange(nrof_images))==True, \'Wrong labels used for evaluation, possibly caused by training examples left in the input pipeline\'\n-    tpr, fpr, accuracy, val, val_std, far = lfw.evaluate(embeddings, actual_issame, nrof_folds=nrof_folds, distance_metric=distance_metric, subtract_mean=subtract_mean)\n-    \n-    print(\'Accuracy: %2.5f+-%2.5f\' % (np.mean(accuracy), np.std(accuracy)))\n-    print(\'Validation rate: %2.5f+-%2.5f @ FAR=%2.5f\' % (val, val_std, far))\n-    \n-    auc = metrics.auc(fpr, tpr)\n-    print(\'Area Under Curve (AUC): %1.3f\' % auc)\n-    eer = brentq(lambda x: 1. - x - interpolate.interp1d(fpr, tpr)(x), 0., 1.)\n-    print(\'Equal Error Rate (EER): %1.3f\' % eer)\n-    \n-def parse_arguments(argv):\n-    parser = argparse.ArgumentParser()\n-    \n-    parser.add_argument(\'lfw_dir\', type=str,\n-        help=\'Path to the data directory containing aligned LFW face patches.\')\n-    parser.add_argument(\'--lfw_batch_size\', type=int,\n-        help=\'Number of images to process in a batch in the LFW test set.\', default=100)\n-    parser.add_argument(\'model\', type=str, \n-        help=\'Could be either a directory containing the meta_file and ckpt_file or a model protobuf (.pb) file\')\n-    parser.add_argument(\'--image_size\', type=int,\n-        help=\'Image size (height, width) in pixels.\', default=160)\n-    parser.add_argument(\'--lfw_pairs\', type=str,\n-        help=\'The file containing the pairs to use for validation.\', default=\'data/pairs.txt\')\n-    parser.add_argument(\'--lfw_nrof_folds\', type=int,\n-        help=\'Number of folds to use for cross validation. Mainly used for testing.\', default=10)\n-    parser.add_argument(\'--distance_metric\', type=int,\n-        help=\'Distance metric  0:euclidian, 1:cosine similarity.\', default=0)\n-    parser.add_argument(\'--use_flipped_images\', \n-        help=\'Concatenates embeddings for the image and its horizontally flipped counterpart.\', action=\'store_true\')\n-    parser.add_argument(\'--subtract_mean\', \n-        help=\'Subtract feature mean before calculating distance.\', action=\'store_true\')\n-    parser.add_argument(\'--use_fixed_image_standardization\', \n-        help=\'Performs fixed standardization of images.\', action=\'store_true\')\n-    return parser.parse_args(argv)\n-\n-if __name__ == \'__main__\':\n-    main(parse_arguments(sys.argv[1:]))'